{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6d8cb26-d00f-4ffc-b2ac-cbd495c13efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, lit\n",
    "from pyspark.sql.functions import col, regexp_replace, trim, when, regexp_extract\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, isnan, when, count ,date_format,to_date,to_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24850141-80bb-4929-a75c-9b9e321cd98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/01/14 18:09:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataProcessing\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a41212c2-79ef-4531-b8f0-e4b3d56c04f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://9fab1b8197c1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>DataProcessing</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7e2b9fb56670>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d56ecf93-f51a-4c3b-934a-55536dd328cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Products Schema\n",
    "products_schema = StructType([\n",
    "     StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b4d1702-b24c-497c-8ea9-5ab175b23f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Customer CSV\n",
    "products_df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(products_schema) \\\n",
    "    .load(\"../data/products.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39ae5507-1c94-4160-bbf1-cb8a3f9f58aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Products DataFrame Schema:\n",
      "root\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check schemas\n",
    "print(\"Products DataFrame Schema:\")\n",
    "products_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0187e04-26bb-4cb2-8ec9-8870519c91cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 100\n"
     ]
    }
   ],
   "source": [
    "num_rows = products_df.count()\n",
    "print(f\"Number of rows: {num_rows}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248e63eb-40c9-499b-9a15-833e08a71023",
   "metadata": {},
   "source": [
    "### Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26674f23-48b4-4cd7-9d36-d45c09847966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/01/14 18:09:53 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: product_name, product_id, category\n",
      " Schema: product_id, product_name, category\n",
      "Expected: product_id but found: product_name\n",
      "CSV file: file:///workspace/data/products.csv\n",
      "+----------+------------+--------+\n",
      "|product_id|product_name|category|\n",
      "+----------+------------+--------+\n",
      "|         0|           0|      16|\n",
      "+----------+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Counting missing values for each column\n",
    "missing_values = products_df.select(\n",
    "    [count(when(col(c).isNull(), c)).alias(c) for c in products_df.columns]\n",
    ")\n",
    "missing_values.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b2edf74-a5af-414d-a38b-b456f7135a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def clean_product_names(products_df):\n",
    "    \"\"\"\n",
    "    Efficiently handle missing names in products dataset using PySpark.\n",
    "    Using na.fill() is more performant than withColumn() for simple replacements.\n",
    "    \n",
    "    Args:\n",
    "        customers_df: PySpark DataFrame containing customer data\n",
    "        \n",
    "    Returns:\n",
    "        PySpark DataFrame with missing names replaced with \"Unknown\"\n",
    "    \"\"\"\n",
    "    # Get list of name columns (assuming they might be first_name, last_name, or name)\n",
    "    name_columns = [col for col in products_df.columns \n",
    "                   if any(name_field in col.lower() \n",
    "                         for name_field in ['product_name','category'])]\n",
    "    \n",
    "    # Create dictionary of columns to fill\n",
    "    fill_dict = {col: \"Unknown\" for col in name_columns}\n",
    "    \n",
    "    # Use na.fill() which is more efficient than withColumn() for simple replacements\n",
    "    cleaned_df = products_df.na.fill(fill_dict)\n",
    "    \n",
    "    # Cache the result if you'll be using it multiple times\n",
    "    cleaned_df = cleaned_df.cache()  # Uncomment if needed\n",
    "    \n",
    "    return cleaned_df\n",
    "\n",
    "\n",
    "# Example usage\n",
    "cleaned_products_df = clean_product_names(products_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68bffc09-f211-4919-a7d7-a161ae533388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/01/14 18:09:53 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: product_name, product_id, category\n",
      " Schema: product_id, product_name, category\n",
      "Expected: product_id but found: product_name\n",
      "CSV file: file:///workspace/data/products.csv\n",
      "+----------+------------+--------+\n",
      "|product_id|product_name|category|\n",
      "+----------+------------+--------+\n",
      "|         0|           0|       0|\n",
      "+----------+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Counting missing values for each column\n",
    "missing_values = cleaned_products_df.select(\n",
    "    [count(when(col(c).isNull(), c)).alias(c) for c in cleaned_products_df.columns]\n",
    ")\n",
    "missing_values.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84674306-dff5-484e-95ce-b0feba9058e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+---------------+\n",
      "|          product_id|product_name|       category|\n",
      "+--------------------+------------+---------------+\n",
      "|    COCA COLA 500ML_|     1001001|      Beverages|\n",
      "|           Flour 2kg|     1005001|      Groceries|\n",
      "|     Coca Cola 500ml|     1001001|InvalidCategory|\n",
      "|         FLOUR 2KG70|     1005001|        Unknown|\n",
      "|INSTANT COFFEE 200G_|     1008002|      Beverages|\n",
      "+--------------------+------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned_products_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0039edf-a3cb-4fab-befc-b8fa28e16d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected column data:\n",
      "25/01/14 18:09:54 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: product_name, product_id, category\n",
      " Schema: product_id, product_name, category\n",
      "Expected: product_id but found: product_name\n",
      "CSV file: file:///workspace/data/products.csv\n",
      "+----------+--------------------+---------------+\n",
      "|product_id|product_name        |category       |\n",
      "+----------+--------------------+---------------+\n",
      "|1001001   |COCA COLA 500ML_    |Beverages      |\n",
      "|1005001   |Flour 2kg           |Groceries      |\n",
      "|1001001   |Coca Cola 500ml     |InvalidCategory|\n",
      "|1005001   |FLOUR 2KG70         |null           |\n",
      "|1008002   |INSTANT COFFEE 200G_|Beverages      |\n",
      "|-99999    |instant coffee 100g_|Beverages      |\n",
      "|1007001   |YOGURT PLAIN 500ML93|Dairy          |\n",
      "|1001003   |Sprite 500ml        |InvalidCategory|\n",
      "|1005002   |Flour 1kg           |Groceries      |\n",
      "|1005001   |FLOUR 2KG           |Groceries      |\n",
      "|-99999    |rice 5kg            |null           |\n",
      "|ID        |tea leaves 250g     |Beverages      |\n",
      "|1006001   |CHEDDAR CHEESE 250G |Dairy          |\n",
      "|1001003   |Sprite 500ml        |null           |\n",
      "|1003002   |Sugar 500g_         |Groceries      |\n",
      "|1005001   |Flour 2kg_          |InvalidCategory|\n",
      "|1004002   |RICE 1KG            |InvalidCategory|\n",
      "|1001003   |sprite 500ml_       |Beverages      |\n",
      "|1007002   |YOGURT PLAIN 250ML  |null           |\n",
      "|1005001   |Flour 2kg69         |Groceries      |\n",
      "+----------+--------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "\n",
    "# Simple and efficient column renaming\n",
    "products_df = products_df.select(\n",
    "    col(\"product_name\").alias(\"product_id\"),\n",
    "    col(\"product_id\").alias(\"product_name\"),\n",
    "    col(\"category\")\n",
    ")\n",
    "\n",
    "# Show results\n",
    "print(\"Corrected column data:\")\n",
    "products_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f45bfde-9db8-4faa-87f9-5839dbbfc9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/01/14 18:09:54 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: product_name, product_id, category\n",
      " Schema: product_id, product_name, category\n",
      "Expected: product_id but found: product_name\n",
      "CSV file: file:///workspace/data/products.csv\n",
      "+----------+--------------------+---------------+\n",
      "|product_id|        product_name|       category|\n",
      "+----------+--------------------+---------------+\n",
      "|   1001001|    COCA COLA 500ML_|      Beverages|\n",
      "|   1005001|           Flour 2kg|      Groceries|\n",
      "|   1001001|     Coca Cola 500ml|InvalidCategory|\n",
      "|   1005001|         FLOUR 2KG70|           null|\n",
      "|   1008002|INSTANT COFFEE 200G_|      Beverages|\n",
      "|    -99999|instant coffee 100g_|      Beverages|\n",
      "|   1007001|YOGURT PLAIN 500ML93|          Dairy|\n",
      "|   1001003|        Sprite 500ml|InvalidCategory|\n",
      "|   1005002|           Flour 1kg|      Groceries|\n",
      "|   1005001|           FLOUR 2KG|      Groceries|\n",
      "|    -99999|            rice 5kg|           null|\n",
      "|        ID|     tea leaves 250g|      Beverages|\n",
      "|   1006001| CHEDDAR CHEESE 250G|          Dairy|\n",
      "|   1001003|        Sprite 500ml|           null|\n",
      "|   1003002|         Sugar 500g_|      Groceries|\n",
      "|   1005001|          Flour 2kg_|InvalidCategory|\n",
      "|   1004002|            RICE 1KG|InvalidCategory|\n",
      "|   1001003|       sprite 500ml_|      Beverages|\n",
      "|   1007002|  YOGURT PLAIN 250ML|           null|\n",
      "|   1005001|         Flour 2kg69|      Groceries|\n",
      "+----------+--------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8fcda92-df20-4075-8fbb-69065759c635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product names in Title Case:\n",
      "25/01/14 18:09:54 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: product_name, product_id, category\n",
      " Schema: product_id, product_name, category\n",
      "Expected: product_id but found: product_name\n",
      "CSV file: file:///workspace/data/products.csv\n",
      "+----------+--------------------+---------------+\n",
      "|product_id|product_name        |category       |\n",
      "+----------+--------------------+---------------+\n",
      "|1001001   |Coca Cola 500ml_    |Beverages      |\n",
      "|1005001   |Flour 2kg           |Groceries      |\n",
      "|1001001   |Coca Cola 500ml     |InvalidCategory|\n",
      "|1005001   |Flour 2kg70         |null           |\n",
      "|1008002   |Instant Coffee 200g_|Beverages      |\n",
      "|-99999    |Instant Coffee 100g_|Beverages      |\n",
      "|1007001   |Yogurt Plain 500ml93|Dairy          |\n",
      "|1001003   |Sprite 500ml        |InvalidCategory|\n",
      "|1005002   |Flour 1kg           |Groceries      |\n",
      "|1005001   |Flour 2kg           |Groceries      |\n",
      "|-99999    |Rice 5kg            |null           |\n",
      "|ID        |Tea Leaves 250g     |Beverages      |\n",
      "|1006001   |Cheddar Cheese 250g |Dairy          |\n",
      "|1001003   |Sprite 500ml        |null           |\n",
      "|1003002   |Sugar 500g_         |Groceries      |\n",
      "|1005001   |Flour 2kg_          |InvalidCategory|\n",
      "|1004002   |Rice 1kg            |InvalidCategory|\n",
      "|1001003   |Sprite 500ml_       |Beverages      |\n",
      "|1007002   |Yogurt Plain 250ml  |null           |\n",
      "|1005001   |Flour 2kg69         |Groceries      |\n",
      "+----------+--------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, initcap\n",
    "\n",
    "# Metadata\n",
    "# Date: 2025-01-14 15:41:07 UTC\n",
    "# User: alexio545\n",
    "\n",
    "# Convert product_name to Title Case while maintaining the column structure\n",
    "products_df = products_df.select(\n",
    "    col(\"product_id\"),\n",
    "    initcap(col(\"product_name\")).alias(\"product_name\"),  # Convert to Title Case\n",
    "    col(\"category\")\n",
    ")\n",
    "\n",
    "# Show results\n",
    "print(\"Product names in Title Case:\")\n",
    "products_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad031535-b44d-4950-a3b6-aed77f5680fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned and standardized products data:\n",
      "25/01/14 18:09:54 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: product_name, product_id, category\n",
      " Schema: product_id, product_name, category\n",
      "Expected: product_id but found: product_name\n",
      "CSV file: file:///workspace/data/products.csv\n",
      "+----------+-------------------+---------+\n",
      "|product_id|product_name       |category |\n",
      "+----------+-------------------+---------+\n",
      "|-99999    |Instant Coffee 100g|Beverages|\n",
      "|123.45    |Flour 2kg          |Groceries|\n",
      "|1001001   |Coca Cola 500ml    |Unknown  |\n",
      "|PROD2351  |Flour 1kg          |Groceries|\n",
      "|1008001   |Instant Coffee 100g|Beverages|\n",
      "|1007001   |Yogurt Plain 500ml |Dairy    |\n",
      "|1007002   |Yogurt Plain 250ml |Unknown  |\n",
      "|1006002   |Cheddar Cheese 500g|Unknown  |\n",
      "|1007001   |Yogurt Plain 500ml |Unknown  |\n",
      "|1006002   |Cheddar Cheese 500g|Dairy    |\n",
      "|1001003   |Sprite 500ml       |Beverages|\n",
      "|1005001   |Flour 2kg          |Groceries|\n",
      "|ID        |Yogurt Plain 250ml |Dairy    |\n",
      "|1004002   |Rice 1kg           |Unknown  |\n",
      "|ID        |Tea Leaves 250g    |Beverages|\n",
      "|1002001   |Cooking Oil 1l     |Unknown  |\n",
      "|-99999    |Cheddar Cheese 250g|Dairy    |\n",
      "|1009002   |Tea Leaves 500g    |Beverages|\n",
      "|1001002   |Pepsi 500ml        |Beverages|\n",
      "|1005002   |Flour 1kg          |Groceries|\n",
      "+----------+-------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "Summary of changes:\n",
      "Original row count: 100\n",
      "25/01/14 18:09:54 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: product_name, product_id, category\n",
      " Schema: product_id, product_name, category\n",
      "Expected: product_id but found: product_name\n",
      "CSV file: file:///workspace/data/products.csv\n",
      "Final row count: 54\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, initcap, regexp_replace, when, lit\n",
    "\n",
    "# Metadata\n",
    "# Date: 2025-01-14 15:51:05 UTC\n",
    "# User: alexio545\n",
    "\n",
    "# Clean and standardize the dataset\n",
    "cleaned_products_df = products_df.select(\n",
    "    # Keep product_id as is\n",
    "    col(\"product_id\"),\n",
    "    \n",
    "    # Clean product_name:\n",
    "    # 1. Remove trailing numbers\n",
    "    # 2. Remove special characters\n",
    "    # 3. Convert to Title Case\n",
    "    # 4. Trim spaces\n",
    "    initcap(\n",
    "        regexp_replace(\n",
    "            regexp_replace(\n",
    "                regexp_replace(\n",
    "                    col(\"product_name\"),\n",
    "                    r'[_\\-]|\\d+$',  # Remove trailing numbers and special characters\n",
    "                    ''\n",
    "                ),\n",
    "                r'\\s+',  # Standardize spaces\n",
    "                ' '\n",
    "            ),\n",
    "            r'\\s+$',  # Remove trailing spaces\n",
    "            ''\n",
    "        )\n",
    "    ).alias(\"product_name\"),\n",
    "    \n",
    "    # Clean category:\n",
    "    # 1. Replace null values with 'Unknown'\n",
    "    # 2. Replace 'InvalidCategory' with 'Unknown'\n",
    "    when(\n",
    "        (col(\"category\").isNull()) | \n",
    "        (col(\"category\") == \"InvalidCategory\"), \n",
    "        lit(\"Unknown\")\n",
    "    ).otherwise(col(\"category\")).alias(\"category\")\n",
    ")\n",
    "\n",
    "# Remove duplicates\n",
    "final_products_df = cleaned_products_df.dropDuplicates([\"product_id\", \"product_name\", \"category\"])\n",
    "\n",
    "# Show results\n",
    "print(\"Cleaned and standardized products data:\")\n",
    "final_products_df.show(truncate=False)\n",
    "\n",
    "# Optional: Count the changes\n",
    "print(\"\\nSummary of changes:\")\n",
    "print(\"Original row count:\", products_df.count())\n",
    "print(\"Final row count:\", final_products_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6aa21ff-3bf0-4d04-8d33-6dbfcc1bd47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hashing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "feba21c3-f214-4f9d-b059-9508aea01bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Products with hashed IDs:\n",
      "25/01/14 18:09:55 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: product_name, category\n",
      " Schema: product_id, category\n",
      "Expected: product_id but found: product_name\n",
      "CSV file: file:///workspace/data/products.csv\n",
      "+--------------------------------+-------------------+---------+\n",
      "|product_id                      |product_name       |category |\n",
      "+--------------------------------+-------------------+---------+\n",
      "|59e9f4bf700031a537a7495085e108c4|Cheddar Cheese 250g|Dairy    |\n",
      "|bd984327e9f06bf46e749d2e1300bb08|Cheddar Cheese 250g|Unknown  |\n",
      "|fde194048278a967cd1e31df11a23052|Cheddar Cheese 500g|Dairy    |\n",
      "|42687c68689652bb63145761b0120672|Cheddar Cheese 500g|Unknown  |\n",
      "|374765b4eb2766da24879be557999ad3|Coca Cola 500ml    |Beverages|\n",
      "|f90fd885f02cb1c92002c744b91dade5|Coca Cola 500ml    |Unknown  |\n",
      "|3c0594ca73dba30ca02ffb2bfadd45c1|Cooking Oil 1l     |Groceries|\n",
      "|15dd4eed8967ffc36ec15da1bb5ec094|Cooking Oil 1l     |Unknown  |\n",
      "|c4ce0dac2a59479a54fd2390ea3aff75|Cooking Oil 500ml  |Groceries|\n",
      "|cced1eca77baae1515d28b3551b2ba5e|Flour 1kg          |Groceries|\n",
      "|1efeef6291721b1b3cea0a8b4a93a1fb|Flour 1kg          |Unknown  |\n",
      "|bdbd13b42313c69b19f2f502f0cd773e|Flour 2kg          |Groceries|\n",
      "|a51df4c002dddadbb081b25c53e28595|Flour 2kg          |Unknown  |\n",
      "|5238265e0712dccef6a797b7ba95fc19|Instant Coffee 100g|Beverages|\n",
      "|f84a6998837a75b512e302618f45a15f|Instant Coffee 100g|Unknown  |\n",
      "|e4c705cd3c24eda69c4448d294a6e822|Instant Coffee 200g|Beverages|\n",
      "|6b28baacd380a0b0fe9a37d51bf14181|Instant Coffee 200g|Unknown  |\n",
      "|05f99abad0a9a5a773c88146e9ff1aee|Pepsi 500ml        |Beverages|\n",
      "|102bf0789865aa068f05948feeb143aa|Rice 1kg           |Groceries|\n",
      "|684e14267ff0d254f26aa3437c443c24|Rice 1kg           |Unknown  |\n",
      "+--------------------------------+-------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "Summary:\n",
      "Original row count: 100\n",
      "Final row count: 36\n",
      "\n",
      "Schema of final dataframe:\n",
      "root\n",
      " |-- product_id: string (nullable = false)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, initcap, regexp_replace, when, lit, \n",
    "    md5, concat, coalesce\n",
    ")\n",
    "\n",
    "# Metadata\n",
    "# Date: 2025-01-14 15:55:34 UTC\n",
    "# User: alexio545\n",
    "\n",
    "# Clean and create hashed product_id\n",
    "cleaned_products_df = products_df.select(\n",
    "    # Clean product_name first\n",
    "    initcap(\n",
    "        regexp_replace(\n",
    "            regexp_replace(\n",
    "                regexp_replace(\n",
    "                    col(\"product_name\"),\n",
    "                    r'[_\\-]|\\d+$',  # Remove trailing numbers and special characters\n",
    "                    ''\n",
    "                ),\n",
    "                r'\\s+',  # Standardize spaces\n",
    "                ' '\n",
    "            ),\n",
    "            r'\\s+$',  # Remove trailing spaces\n",
    "            ''\n",
    "        )\n",
    "    ).alias(\"product_name\"),\n",
    "    \n",
    "    # Clean category\n",
    "    when(\n",
    "        (col(\"category\").isNull()) | \n",
    "        (col(\"category\") == \"InvalidCategory\"), \n",
    "        lit(\"Unknown\")\n",
    "    ).otherwise(col(\"category\")).alias(\"category\")\n",
    ")\n",
    "\n",
    "# Create hashed product_id\n",
    "final_products_df = cleaned_products_df.withColumn(\n",
    "    \"product_id\",\n",
    "    md5(concat(\n",
    "        coalesce(col(\"product_name\"), lit(\"\")),\n",
    "        lit(\"_\"),  # Delimiter for better uniqueness\n",
    "        coalesce(col(\"category\"), lit(\"\"))\n",
    "    ))\n",
    ").select(\n",
    "    \"product_id\",\n",
    "    \"product_name\",\n",
    "    \"category\"\n",
    ")\n",
    "\n",
    "# Cache the result for better performance\n",
    "final_products_df.cache()\n",
    "\n",
    "# Remove duplicates\n",
    "final_products_df = final_products_df.dropDuplicates([\"product_name\", \"category\"])\n",
    "\n",
    "# Show results\n",
    "print(\"Products with hashed IDs:\")\n",
    "final_products_df.show(truncate=False)\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nSummary:\")\n",
    "print(\"Original row count:\", products_df.count())\n",
    "print(\"Final row count:\", final_products_df.count())\n",
    "print(\"\\nSchema of final dataframe:\")\n",
    "final_products_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230f8838-1697-48e0-9606-9a8d54eb6c54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba31c56e-1072-4a37-a6a0-10391464f94f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
