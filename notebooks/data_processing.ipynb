{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, lit\n",
    "from pyspark.sql.functions import col, regexp_replace, trim, when, regexp_extract\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, isnan, when, count ,date_format,to_date,to_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/01/14 12:11:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataProcessing\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://12f48e005de8:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>DataProcessing</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x770bd8795460>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Customers Schema\n",
    "customers_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"customer_name\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Customer CSV\n",
    "customers_df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(customers_schema) \\\n",
    "    .load(\"../data/customers.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customers DataFrame Schema:\n",
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check schemas\n",
    "print(\"Customers DataFrame Schema:\")\n",
    "customers_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Checkin the number of rows in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 35\n"
     ]
    }
   ],
   "source": [
    "num_rows = customers_df.count()\n",
    "print(f\"Number of rows: {num_rows}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----+\n",
      "|customer_id|customer_name|city|\n",
      "+-----------+-------------+----+\n",
      "|          0|            0|   0|\n",
      "+-----------+-------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Counting missing values for each column\n",
    "missing_values = customers_df.select(\n",
    "    [count(when(col(c).isNull(), c)).alias(c) for c in customers_df.columns]\n",
    ")\n",
    "missing_values.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping  null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hosts_df = customers_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----+\n",
      "|customer_id|customer_name|city|\n",
      "+-----------+-------------+----+\n",
      "|          0|            0|   0|\n",
      "+-----------+-------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Counting missing values for each column\n",
    "missing_values = customers_df.select(\n",
    "    [count(when(col(c).isNull(), c)).alias(c) for c in customers_df.columns]\n",
    ")\n",
    "missing_values.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+---------+\n",
      "|customer_id|    customer_name|     city|\n",
      "+-----------+-----------------+---------+\n",
      "|     789201|        Rel Fresh|    Surat|\n",
      "|     789202|        Rel Fresh|Ahmedabad|\n",
      "|     789203|        Rel Fresh| Vadodara|\n",
      "|     789301|Expression Stores|    Surat|\n",
      "|     789303|Expression Stores| Vadodara|\n",
      "+-----------+-----------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Duplicate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Checking for duplicates based on customer_id\n",
    "duplicate_count = customers_df.count() - customers_df.dropDuplicates([\"customer_id\"]).count()\n",
    "\n",
    "# Displaying the number of duplicate rows\n",
    "print(f\"Number of duplicate rows: {duplicate_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  There are no duplicate values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Type Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check the data type\n",
    "customers_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating  date and time with formatted date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hosts_df = (\n",
    "    hosts_df\n",
    "    .withColumn(\"date_created\", to_date(col(\"created_at\"), \"yyyy-MM-dd\"))\n",
    "    .withColumn(\"time_created\", date_format(col(\"created_at\"), \"HH:mm:ss\"))\n",
    "\n",
    "    .withColumn(\"date_updated\", to_date(col(\"updated_at\"), \"yyyy-MM-dd\"))\n",
    "    .withColumn(\"time_updated\", date_format(col(\"updated_at\"), \"HH:mm:ss\"))\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------+------------+------------+------------+\n",
      "|         created_at|         updated_at|date_created|time_created|date_updated|time_updated|\n",
      "+-------------------+-------------------+------------+------------+------------+------------+\n",
      "|2014-01-05 16:12:45|2014-01-05 16:12:45|  2014-01-05|    16:12:45|  2014-01-05|    16:12:45|\n",
      "|2013-07-31 23:29:31|2013-07-31 23:29:31|  2013-07-31|    23:29:31|  2013-07-31|    23:29:31|\n",
      "|2017-10-17 05:20:28|2017-10-17 05:20:28|  2017-10-17|    05:20:28|  2017-10-17|    05:20:28|\n",
      "|2009-06-05 21:34:42|2009-06-05 21:34:42|  2009-06-05|    21:34:42|  2009-06-05|    21:34:42|\n",
      "|2021-10-24 02:42:09|2021-10-24 02:42:09|  2021-10-24|    02:42:09|  2021-10-24|    02:42:09|\n",
      "+-------------------+-------------------+------------+------------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Checking the transfformed data\n",
    "selected_columns = ['created_at', 'updated_at','date_created','time_created', 'date_updated', 'time_updated']\n",
    "hosts_df.select(selected_columns).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting is_superhost   from String to Boolean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting  is_superhost to boolean\n",
    "hosts_df = hosts_df.withColumn(\n",
    "    \"is_superhost\",\n",
    "    when(col(\"is_superhost\") == \"t\", True).when(col(\"is_superhost\") == \"f\", False).otherwise(None)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------------+-------------------+-------------------+------------+------------+------------+------------+\n",
      "|id   |name   |is_superhost|created_at         |updated_at         |date_created|time_created|date_updated|time_updated|\n",
      "+-----+-------+------------+-------------------+-------------------+------------+------------+------------+------------+\n",
      "|1581 |Annette|false       |2014-01-05 16:12:45|2014-01-05 16:12:45|2014-01-05  |16:12:45    |2014-01-05  |16:12:45    |\n",
      "|2164 |Lulah  |true        |2013-07-31 23:29:31|2013-07-31 23:29:31|2013-07-31  |23:29:31    |2013-07-31  |23:29:31    |\n",
      "|2217 |Ion    |true        |2017-10-17 05:20:28|2017-10-17 05:20:28|2017-10-17  |05:20:28    |2017-10-17  |05:20:28    |\n",
      "|3718 |Britta |false       |2009-06-05 21:34:42|2009-06-05 21:34:42|2009-06-05  |21:34:42    |2009-06-05  |21:34:42    |\n",
      "|11622|Maria  |false       |2021-10-24 02:42:09|2021-10-24 02:42:09|2021-10-24  |02:42:09    |2021-10-24  |02:42:09    |\n",
      "+-----+-------+------------+-------------------+-------------------+------------+------------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hosts_df.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- is_superhost: boolean (nullable = true)\n",
      " |-- created_at: timestamp (nullable = true)\n",
      " |-- updated_at: timestamp (nullable = true)\n",
      " |-- date_created: date (nullable = true)\n",
      " |-- time_created: string (nullable = true)\n",
      " |-- date_updated: date (nullable = true)\n",
      " |-- time_updated: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hosts_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   2. Listings Data Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listings schema\n",
    "listings_schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"listing_url\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"room_type\", StringType(), True),\n",
    "    StructField(\"minimum_nights\", IntegerType(), True),\n",
    "    StructField(\"host_id\", StringType(), True),\n",
    "    StructField(\"price\", StringType(), True),\n",
    "    StructField(\"created_at\", TimestampType(), True),\n",
    "    StructField(\"updated_at\", TimestampType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Listings Data\n",
    "listings_df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(listings_schema) \\\n",
    "    .load(\"../data/listings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listings DataFrame Schema:\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- listing_url: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- room_type: string (nullable = true)\n",
      " |-- minimum_nights: integer (nullable = true)\n",
      " |-- host_id: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- created_at: timestamp (nullable = true)\n",
      " |-- updated_at: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check schemas\n",
    "print(\"Listings DataFrame Schema:\")\n",
    "listings_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------------------------+-----------------------------------+---------------+--------------+-------+-------+-------------------+-------------------+\n",
      "|id   |listing_url                       |name                               |room_type      |minimum_nights|host_id|price  |created_at         |updated_at         |\n",
      "+-----+----------------------------------+-----------------------------------+---------------+--------------+-------+-------+-------------------+-------------------+\n",
      "|3176 |https://www.airbnb.com/rooms/3176 |Fabulous Flat in great Location    |Entire home/apt|62            |3718   |$90.00 |2009-06-05 21:34:42|2009-06-05 21:34:42|\n",
      "|7071 |https://www.airbnb.com/rooms/7071 |BrightRoom with sunny greenview!   |Private room   |1             |17391  |$33.00 |2009-08-12 12:30:30|2009-08-12 12:30:30|\n",
      "|9991 |https://www.airbnb.com/rooms/9991 |Geourgeous flat - outstanding views|Entire home/apt|0             |33852  |$180.00|2015-07-30 05:08:52|2015-07-30 05:08:52|\n",
      "|14325|https://www.airbnb.com/rooms/14325|Apartment in Prenzlauer Berg       |Entire home/apt|95            |55531  |$70.00 |2010-06-15 19:56:01|2010-06-15 19:56:01|\n",
      "|16644|https://www.airbnb.com/rooms/16644|In the Heart of Berlin - Kreuzberg |Entire home/apt|60            |64696  |$90.00 |2010-05-30 12:11:33|2010-05-30 12:11:33|\n",
      "+-----+----------------------------------+-----------------------------------+---------------+--------------+-------+-------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "listings_df.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Checking the number of rows in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 17499\n"
     ]
    }
   ],
   "source": [
    "num_rows = listings_df.count()\n",
    "print(f\"Number of rows: {num_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+----+---------+--------------+-------+-----+----------+----------+\n",
      "| id|listing_url|name|room_type|minimum_nights|host_id|price|created_at|updated_at|\n",
      "+---+-----------+----+---------+--------------+-------+-----+----------+----------+\n",
      "|  0|          0|   0|        0|            59|      0|    0|        59|         0|\n",
      "+---+-----------+----+---------+--------------+-------+-----+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count missing values for each column\n",
    "missing_values = listings_df.select(\n",
    "    [count(when(col(c).isNull(), c)).alias(c) for c in listings_df.columns]\n",
    ")\n",
    "missing_values.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are  missing values in  minimum_nights  and created_at\n",
    "- Since the values are few , we can just dropp them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_df = listings_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+----+---------+--------------+-------+-----+----------+----------+\n",
      "| id|listing_url|name|room_type|minimum_nights|host_id|price|created_at|updated_at|\n",
      "+---+-----------+----+---------+--------------+-------+-----+----------+----------+\n",
      "|  0|          0|   0|        0|             0|      0|    0|         0|         0|\n",
      "+---+-----------+----+---------+--------------+-------+-----+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Counting  missing values for each column\n",
    "missing_values = listings_df.select(\n",
    "    [count(when(col(c).isNull(), c)).alias(c) for c in listings_df.columns]\n",
    ")\n",
    "missing_values.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All the missing values have been dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Checking for duplicates based on id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "duplicate_count = listings_df.count() - listings_df.dropDuplicates([\"id\"]).count()\n",
    "\n",
    "# Displaying the number of duplicate rows\n",
    "print(f\"Number of duplicate rows: {duplicate_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are no duplicate values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering  out invalid data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Removing $ on Price and converting it to a  double data type  \n",
    "-  Excluding  rows with invalid or nonsensical values eg   where price is zero or negative.\n",
    "and minimum nights is greater than a year (365 days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and convert price column\n",
    "listings_df = (\n",
    "    listings_df\n",
    "    .withColumn(\"price\", \n",
    "        regexp_extract(\n",
    "            regexp_replace(\n",
    "                trim(col(\"price\")), \n",
    "                \"^\\\\$\", \"\"  # Remove leading $ sign\n",
    "            ), \n",
    "            \"^\\\\d+(\\\\.\\\\d+)?\", 0  # Extract numeric part\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\"price\", \n",
    "        when(\n",
    "            col(\"price\").rlike(\"^\\\\d+(\\\\.\\\\d+)?$\"),  # Validate numeric format\n",
    "            col(\"price\").cast(\"double\")\n",
    "        ).otherwise(None)  # Handle non-numeric entries\n",
    "    )\n",
    "    .filter(col(\"price\").isNotNull())\n",
    "    .filter((col(\"price\") > 0) & (col(\"price\") <= 10000))  # Optional price range filter\n",
    "    .filter((col(\"minimum_nights\") > 0) & (col(\"minimum_nights\") <= 365))\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------------------------+---------------------------------------------+---------------+--------------+-------+-----+-------------------+-------------------+\n",
      "|id   |listing_url                       |name                                         |room_type      |minimum_nights|host_id|price|created_at         |updated_at         |\n",
      "+-----+----------------------------------+---------------------------------------------+---------------+--------------+-------+-----+-------------------+-------------------+\n",
      "|3176 |https://www.airbnb.com/rooms/3176 |Fabulous Flat in great Location              |Entire home/apt|62            |3718   |90.0 |2009-06-05 21:34:42|2009-06-05 21:34:42|\n",
      "|7071 |https://www.airbnb.com/rooms/7071 |BrightRoom with sunny greenview!             |Private room   |1             |17391  |33.0 |2009-08-12 12:30:30|2009-08-12 12:30:30|\n",
      "|14325|https://www.airbnb.com/rooms/14325|Apartment in Prenzlauer Berg                 |Entire home/apt|95            |55531  |70.0 |2010-06-15 19:56:01|2010-06-15 19:56:01|\n",
      "|16644|https://www.airbnb.com/rooms/16644|In the Heart of Berlin - Kreuzberg           |Entire home/apt|60            |64696  |90.0 |2010-05-30 12:11:33|2010-05-30 12:11:33|\n",
      "|17904|https://www.airbnb.com/rooms/17904|Beautiful Kreuzberg studio - 3 months minimum|Entire home/apt|92            |68997  |47.0 |2010-02-08 17:23:48|2010-02-08 17:23:48|\n",
      "+-----+----------------------------------+---------------------------------------------+---------------+--------------+-------+-----+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "listings_df.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- listing_url: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- room_type: string (nullable = true)\n",
      " |-- minimum_nights: integer (nullable = true)\n",
      " |-- host_id: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- created_at: timestamp (nullable = true)\n",
      " |-- updated_at: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "listings_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Type Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating  date and time with formatted date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_df = (\n",
    "    listings_df\n",
    "    .withColumn(\"date_created\", to_date(col(\"created_at\"), \"yyyy-MM-dd\"))\n",
    "    .withColumn(\"time_created\", date_format(col(\"created_at\"), \"HH:mm:ss\"))\n",
    "\n",
    "    .withColumn(\"date_updated\", to_date(col(\"updated_at\"), \"yyyy-MM-dd\"))\n",
    "    .withColumn(\"time_updated\", date_format(col(\"updated_at\"), \"HH:mm:ss\"))\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------+------------+------------+------------+\n",
      "|created_at         |updated_at         |date_created|time_created|date_updated|time_updated|\n",
      "+-------------------+-------------------+------------+------------+------------+------------+\n",
      "|2009-06-05 21:34:42|2009-06-05 21:34:42|2009-06-05  |21:34:42    |2009-06-05  |21:34:42    |\n",
      "|2009-08-12 12:30:30|2009-08-12 12:30:30|2009-08-12  |12:30:30    |2009-08-12  |12:30:30    |\n",
      "|2010-06-15 19:56:01|2010-06-15 19:56:01|2010-06-15  |19:56:01    |2010-06-15  |19:56:01    |\n",
      "|2010-05-30 12:11:33|2010-05-30 12:11:33|2010-05-30  |12:11:33    |2010-05-30  |12:11:33    |\n",
      "|2010-02-08 17:23:48|2010-02-08 17:23:48|2010-02-08  |17:23:48    |2010-02-08  |17:23:48    |\n",
      "+-------------------+-------------------+------------+------------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected_columns = ['created_at', 'updated_at','date_created','time_created', 'date_updated', 'time_updated']\n",
    "listings_df.select(selected_columns).show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Reviews  Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reviews Schema\n",
    "reviews_schema = StructType([\n",
    "    StructField(\"listing_id\", StringType(), True),\n",
    "    StructField(\"date\", TimestampType(), True),\n",
    "    StructField(\"reviewer_name\", StringType(), True),\n",
    "    StructField(\"comments\", StringType(), True),\n",
    "    StructField(\"sentiment\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Reviews Data\n",
    "reviews_df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(reviews_schema) \\\n",
    "    .load(\"../data/reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews DataFrame Schema:\n",
      "root\n",
      " |-- listing_id: string (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- reviewer_name: string (nullable = true)\n",
      " |-- comments: string (nullable = true)\n",
      " |-- sentiment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking the  schema\n",
    "print(\"Reviews DataFrame Schema:\")\n",
    "reviews_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------+--------------------+---------+\n",
      "|listing_id|               date|reviewer_name|            comments|sentiment|\n",
      "+----------+-------------------+-------------+--------------------+---------+\n",
      "|      3176|2009-06-20 00:00:00|        Milan|excellent stay, i...| positive|\n",
      "|      3176|2010-11-07 00:00:00|       George|Brittas apartment...| positive|\n",
      "|      3176|2010-11-24 00:00:00|     Patricia|Fantastic, large ...| positive|\n",
      "|      3176|2010-12-21 00:00:00|    Benedetta|Lappartamento di ...|  neutral|\n",
      "|      3176|2011-01-04 00:00:00|         Aude|We went in Berlin...| positive|\n",
      "+----------+-------------------+-------------+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Checking the number of rows in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 410284\n"
     ]
    }
   ],
   "source": [
    "num_rows = reviews_df.count()\n",
    "print(f\"Number of rows: {num_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:>                                                         (0 + 4) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-------------+--------+---------+\n",
      "|listing_id|date|reviewer_name|comments|sentiment|\n",
      "+----------+----+-------------+--------+---------+\n",
      "|         0|   0|            0|     587|      385|\n",
      "+----------+----+-------------+--------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Count missing values for each column\n",
    "missing_values = reviews_df.select(\n",
    "    [count(when(col(c).isNull(), c)).alias(c) for c in reviews_df.columns]\n",
    ")\n",
    "missing_values.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As  shown above the missing values are  in non-critical columns (comments or sentiment)\n",
    "- In this case the missing values will be  replaced by placeholders (e.g., \"Unknown\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing unknown values where   the data is missing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute \"Unknown\" for missing values in comments and sentiment columns\n",
    "reviews_df = (\n",
    "    reviews_df\n",
    "    .withColumn(\"comments\", \n",
    "        when(col(\"comments\").isNull() | (col(\"comments\") == \"\"), \n",
    "             lit(\"Unknown\"))\n",
    "        .otherwise(col(\"comments\"))\n",
    "    )\n",
    "    .withColumn(\"sentiment\", \n",
    "        when(col(\"sentiment\").isNull() | (col(\"sentiment\") == \"\"), \n",
    "             lit(\"Unknown\"))\n",
    "        .otherwise(col(\"sentiment\"))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-------------+--------+---------+\n",
      "|listing_id|date|reviewer_name|comments|sentiment|\n",
      "+----------+----+-------------+--------+---------+\n",
      "|         0|   0|            0|       0|        0|\n",
      "+----------+----+-------------+--------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Counting missing values for each column\n",
    "missing_values = reviews_df.select(\n",
    "    [count(when(col(c).isNull(), c)).alias(c) for c in reviews_df.columns]\n",
    ")\n",
    "missing_values.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the date column into day and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = (\n",
    "    reviews_df\n",
    "    .withColumn(\"date1\", to_date(col(\"date\"), \"yyyy-MM-dd\"))\n",
    "    .withColumn(\"time\", date_format(col(\"date\"), \"HH:mm:ss\"))\n",
    "    .drop(\"date\")\n",
    "    .withColumnRenamed(\"date1\", \"date\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+---------+----------+--------+\n",
      "|listing_id|reviewer_name|            comments|sentiment|      date|    time|\n",
      "+----------+-------------+--------------------+---------+----------+--------+\n",
      "|      3176|        Milan|excellent stay, i...| positive|2009-06-20|00:00:00|\n",
      "|      3176|       George|Brittas apartment...| positive|2010-11-07|00:00:00|\n",
      "|      3176|     Patricia|Fantastic, large ...| positive|2010-11-24|00:00:00|\n",
      "|      3176|    Benedetta|Lappartamento di ...|  neutral|2010-12-21|00:00:00|\n",
      "|      3176|         Aude|We went in Berlin...| positive|2011-01-04|00:00:00|\n",
      "+----------+-------------+--------------------+---------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- listing_id: string (nullable = true)\n",
      " |-- reviewer_name: string (nullable = true)\n",
      " |-- comments: string (nullable = true)\n",
      " |-- sentiment: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, when, lit, regexp_replace, trim, \n",
    "    regexp_extract, isnan, count, date_format,\n",
    "    to_date, to_timestamp\n",
    ")\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def create_spark_session(app_name=\"DataProcessing\"):\n",
    "    \"\"\"\n",
    "    Create a SparkSession.\n",
    "\n",
    "    Args:\n",
    "        app_name (str): The name of the Spark application.\n",
    "\n",
    "    Returns:\n",
    "        SparkSession: A Spark session object.\n",
    "    \"\"\"\n",
    "    return SparkSession.builder.appName(app_name).getOrCreate()\n",
    "\n",
    "def read_csv_with_schema(spark, file_path, schema):\n",
    "    \"\"\"\n",
    "    Read a CSV file with a specified schema.\n",
    "\n",
    "    Args:\n",
    "        spark (SparkSession): The Spark session.\n",
    "        file_path (str): Path to the CSV file.\n",
    "        schema (StructType): Schema to apply to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame containing the data.\n",
    "    \"\"\"\n",
    "    return spark.read.format(\"csv\").option(\"header\", True).schema(schema).load(file_path)\n",
    "\n",
    "def check_missing_values(df):\n",
    "    \"\"\"\n",
    "    Check for missing values in each column.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame showing the count of missing values for each column.\n",
    "    \"\"\"\n",
    "    return df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "\n",
    "def handle_missing_values(df, strategy=\"drop\"):\n",
    "    \"\"\"\n",
    "    Handle missing values in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input DataFrame.\n",
    "        strategy (str): Strategy to handle missing values ('drop' or 'retain').\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The DataFrame after handling missing values.\n",
    "    \"\"\"\n",
    "    if strategy == \"drop\":\n",
    "        return df.dropna()\n",
    "    return df\n",
    "\n",
    "def check_duplicates(df, id_column):\n",
    "    \"\"\"\n",
    "    Check for duplicate rows based on a specific column.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input DataFrame.\n",
    "        id_column (str): The column used to identify duplicates.\n",
    "\n",
    "    Returns:\n",
    "        int: The count of duplicate rows.\n",
    "    \"\"\"\n",
    "    duplicate_count = df.count() - df.dropDuplicates([id_column]).count()\n",
    "    return duplicate_count\n",
    "\n",
    "def transform_datetime_columns(df, timestamp_columns):\n",
    "    \"\"\"\n",
    "    Split timestamp columns into date and time components.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input DataFrame.\n",
    "        timestamp_columns (list): List of column names containing timestamps.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The DataFrame with split date and time components.\n",
    "    \"\"\"\n",
    "    for col_name in timestamp_columns:\n",
    "        df = (df\n",
    "            .withColumn(f\"date_{col_name}\", to_date(col(col_name), \"yyyy-MM-dd\"))\n",
    "            .withColumn(f\"time_{col_name}\", date_format(col(col_name), \"HH:mm:ss\"))\n",
    "        )\n",
    "    return df\n",
    "\n",
    "def clean_price_column(df):\n",
    "    \"\"\"\n",
    "    Clean and validate the price column.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The DataFrame with a cleaned and validated price column.\n",
    "    \"\"\"\n",
    "    return (df\n",
    "        .withColumn(\"price\", \n",
    "            regexp_extract(\n",
    "                regexp_replace(trim(col(\"price\")), \"^\\\\$\", \"\"),\n",
    "                \"^\\\\d+(\\\\.\\\\d+)?\", 0\n",
    "            )\n",
    "        )\n",
    "        .withColumn(\"price\", \n",
    "            when(\n",
    "                col(\"price\").rlike(\"^\\\\d+(\\\\.\\\\d+)?$\"),\n",
    "                col(\"price\").cast(\"double\")\n",
    "            ).otherwise(None)\n",
    "        )\n",
    "        .filter(col(\"price\").isNotNull())\n",
    "        .filter((col(\"price\") > 0) & (col(\"price\") <= 10000))\n",
    "    )\n",
    "\n",
    "def validate_listings(df):\n",
    "    \"\"\"\n",
    "    Validate the listings based on minimum nights.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The DataFrame after filtering valid listings.\n",
    "    \"\"\"\n",
    "    return df.filter((col(\"minimum_nights\") > 0) & (col(\"minimum_nights\") <= 365))\n",
    "\n",
    "def convert_superhost_to_boolean(df):\n",
    "    \"\"\"\n",
    "    Convert the 'is_superhost' column to boolean.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The DataFrame with a boolean 'is_superhost' column.\n",
    "    \"\"\"\n",
    "    return df.withColumn(\n",
    "        \"is_superhost\",\n",
    "        when(col(\"is_superhost\") == \"t\", True)\n",
    "        .when(col(\"is_superhost\") == \"f\", False)\n",
    "        .otherwise(None)\n",
    "    )\n",
    "\n",
    "def impute_text_columns(df, columns, default_value=\"Unknown\"):\n",
    "    \"\"\"\n",
    "    Impute missing text columns with a default value.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input DataFrame.\n",
    "        columns (list): List of column names to impute.\n",
    "        default_value (str): The default value for missing text.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The DataFrame with imputed text columns.\n",
    "    \"\"\"\n",
    "    for column in columns:\n",
    "        df = df.withColumn(\n",
    "            column,\n",
    "            when(col(column).isNull() | (col(column) == \"\"), lit(default_value))\n",
    "            .otherwise(col(column))\n",
    "        )\n",
    "    return df\n",
    "\n",
    "def process_hosts_data(spark, file_path):\n",
    "    \"\"\"\n",
    "    Process host data from a CSV file.\n",
    "\n",
    "    Args:\n",
    "        spark (SparkSession): The Spark session.\n",
    "        file_path (str): Path to the hosts CSV file.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The processed host data.\n",
    "    \"\"\"\n",
    "    hosts_schema = StructType([\n",
    "        StructField(\"id\", StringType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"is_superhost\", StringType(), True),\n",
    "        StructField(\"created_at\", TimestampType(), True),\n",
    "        StructField(\"updated_at\", TimestampType(), True)\n",
    "    ])\n",
    "    \n",
    "    df = (read_csv_with_schema(spark, file_path, hosts_schema)\n",
    "          .transform(lambda df: handle_missing_values(df))\n",
    "          .transform(convert_superhost_to_boolean)\n",
    "          .transform(lambda df: transform_datetime_columns(df, [\"created_at\", \"updated_at\"])))\n",
    "    return df\n",
    "\n",
    "def process_listings_data(spark, file_path):\n",
    "    \"\"\"\n",
    "    Process listing data from a CSV file.\n",
    "\n",
    "    Args:\n",
    "        spark (SparkSession): The Spark session.\n",
    "        file_path (str): Path to the listings CSV file.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The processed listing data.\n",
    "    \"\"\"\n",
    "    listings_schema = StructType([\n",
    "        StructField(\"id\", StringType(), True),\n",
    "        StructField(\"listing_url\", StringType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"room_type\", StringType(), True),\n",
    "        StructField(\"minimum_nights\", IntegerType(), True),\n",
    "        StructField(\"host_id\", StringType(), True),\n",
    "        StructField(\"price\", StringType(), True),\n",
    "        StructField(\"created_at\", TimestampType(), True),\n",
    "        StructField(\"updated_at\", TimestampType(), True)\n",
    "    ])\n",
    "    \n",
    "    df = (read_csv_with_schema(spark, file_path, listings_schema)\n",
    "          .transform(lambda df: handle_missing_values(df))\n",
    "          .transform(clean_price_column)\n",
    "          .transform(validate_listings)\n",
    "          .transform(lambda df: transform_datetime_columns(df, [\"created_at\", \"updated_at\"])))\n",
    "    return df\n",
    "\n",
    "def process_reviews_data(spark, file_path):\n",
    "    \"\"\"\n",
    "    Process review data from a CSV file.\n",
    "\n",
    "    Args:\n",
    "        spark (SparkSession): The Spark session.\n",
    "        file_path (str): Path to the reviews CSV file.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The processed review data.\n",
    "    \"\"\"\n",
    "    reviews_schema = StructType([\n",
    "        StructField(\"listing_id\", StringType(), True),\n",
    "        StructField(\"date\", TimestampType(), True),\n",
    "        StructField(\"reviewer_name\", StringType(), True),\n",
    "        StructField(\"comments\", StringType(), True),\n",
    "        StructField(\"sentiment\", StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    df = (read_csv_with_schema(spark, file_path, reviews_schema)\n",
    "          .transform(lambda df: impute_text_columns(df, [\"comments\", \"sentiment\"]))\n",
    "          .transform(lambda df: transform_datetime_columns(df, [\"date\"])))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
