{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3acf594-f4fd-4924-aebf-cb6a0305b097",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, lit\n",
    "from pyspark.sql.functions import col, regexp_replace, trim, when, regexp_extract\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, isnan, when, count ,date_format,to_date,to_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "291f1532-41cf-49d9-b074-a2b485ce5d00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/01/14 18:10:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/01/14 18:10:34 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Create Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataProcessing\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f30670f-3e59-45fc-bc00-7b66ef17ab3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://9fab1b8197c1:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>DataProcessing</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x710000279460>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81073ba6-1894-41c5-843a-b7ec8d5644e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DateType, IntegerType\n",
    "\n",
    "date_schema = StructType([\n",
    "    StructField(\"date\", DateType(), True),       # For actual date values (e.g., \"2023-01-01\")\n",
    "    StructField(\"mmm_yy\", StringType(), True),  # For month and year as a string (e.g., \"Jan-23\")\n",
    "    StructField(\"week_no\", StringType(), True) # For week number as an integer (e.g., 1, 2, 3, ...)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "daa2c5d7-ce39-4ab7-a5ec-e96344064374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Customer CSV\n",
    "date_df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(date_schema) \\\n",
    "    .load(\"../data/dates.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1dc3a31c-9a4b-4b8d-a23c-b1ddbec7e219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dates DataFrame Schema:\n",
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- mmm_yy: string (nullable = true)\n",
      " |-- week_no: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check schemas\n",
    "print(\"Dates DataFrame Schema:\")\n",
    "date_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b81dce38-c5b3-42c9-be72-ed7392f8c2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 366\n"
     ]
    }
   ],
   "source": [
    "num_rows = date_df.count()\n",
    "print(f\"Number of rows: {num_rows}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d58a90e-ed94-4f3b-babb-7a2cc6b47c96",
   "metadata": {},
   "source": [
    "### Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "946ab91d-05c6-45ec-b345-e2d44be0fe15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+\n",
      "|date|mmm_yy|week_no|\n",
      "+----+------+-------+\n",
      "|  42|    41|      0|\n",
      "+----+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Counting missing values for each column\n",
    "missing_values = date_df.select(\n",
    "    [count(when(col(c).isNull(), c)).alias(c) for c in date_df.columns]\n",
    ")\n",
    "missing_values.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c93dc23-a531-4f08-a2be-60e0b79ffe6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-------+\n",
      "|      date|      mmm_yy|week_no|\n",
      "+----------+------------+-------+\n",
      "|2024-01-01|      Jan-24|     W1|\n",
      "|2024-01-02|      Jan-24|     w1|\n",
      "|2024-01-03|      Jan-24|     W1|\n",
      "|2024-01-04|      Jan-24|     W1|\n",
      "|2024-01-05|      Jan-24|     W1|\n",
      "|2024-01-06|      Jan-24|     W1|\n",
      "|2024-01-07|        null|     W1|\n",
      "|2024-01-08|      JAN-24|     W2|\n",
      "|2024-01-09|      Jan-24|     w2|\n",
      "|2024-01-10|      Jan-24|     W2|\n",
      "|2024-01-11|      Jan-24|     W2|\n",
      "|2024-01-12|      JAN-24|     W2|\n",
      "|2024-01-13|      Jan-24|     W2|\n",
      "|2024-01-14|      Jan-24|     W2|\n",
      "|2024-01-15|      JAN-24|     W3|\n",
      "|2024-01-16|      Jan-24|     w3|\n",
      "|2024-01-17|      Jan-24|     w3|\n",
      "|2024-01-18|invalid_date|     W3|\n",
      "|2024-01-19|      Jan-24|     W3|\n",
      "|2024-01-20|      Jan-24|     W3|\n",
      "+----------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "692d2958-e6b8-42c7-9891-c4a772f303f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original row count: 366\n",
      "\n",
      "Cleaned data (rows with no missing values):\n",
      "+----------+------------+-------+\n",
      "|date      |mmm_yy      |week_no|\n",
      "+----------+------------+-------+\n",
      "|2024-01-01|Jan-24      |W1     |\n",
      "|2024-01-02|Jan-24      |w1     |\n",
      "|2024-01-03|Jan-24      |W1     |\n",
      "|2024-01-04|Jan-24      |W1     |\n",
      "|2024-01-05|Jan-24      |W1     |\n",
      "|2024-01-06|Jan-24      |W1     |\n",
      "|2024-01-08|JAN-24      |W2     |\n",
      "|2024-01-09|Jan-24      |w2     |\n",
      "|2024-01-10|Jan-24      |W2     |\n",
      "|2024-01-11|Jan-24      |W2     |\n",
      "|2024-01-12|JAN-24      |W2     |\n",
      "|2024-01-13|Jan-24      |W2     |\n",
      "|2024-01-14|Jan-24      |W2     |\n",
      "|2024-01-15|JAN-24      |W3     |\n",
      "|2024-01-16|Jan-24      |w3     |\n",
      "|2024-01-17|Jan-24      |w3     |\n",
      "|2024-01-18|invalid_date|W3     |\n",
      "|2024-01-19|Jan-24      |W3     |\n",
      "|2024-01-20|Jan-24      |W3     |\n",
      "|2024-01-21|JAN-24      |w3     |\n",
      "+----------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "Summary of changes:\n",
      "Original row count: 286\n",
      "Final row count after dropping missing values: 286\n",
      "\n",
      "Schema of cleaned dataframe:\n",
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- mmm_yy: string (nullable = true)\n",
      " |-- week_no: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show initial count\n",
    "print(\"Original row count:\", date_df.count())\n",
    "\n",
    "# Drop all rows with any null/missing values\n",
    "date_df = date_df.na.drop(how='any')\n",
    "\n",
    "# Show results and final count\n",
    "print(\"\\nCleaned data (rows with no missing values):\")\n",
    "date_df.show(truncate=False)\n",
    "\n",
    "print(\"\\nSummary of changes:\")\n",
    "print(\"Original row count:\", date_df.count())\n",
    "print(\"Final row count after dropping missing values:\", cleaned_date_df.count())\n",
    "\n",
    "# Optional: Show the schema to verify columns\n",
    "print(\"\\nSchema of cleaned dataframe:\")\n",
    "date_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c17bdcdb-c88c-4f3d-af21-8fb986489e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+\n",
      "|date|mmm_yy|week_no|\n",
      "+----+------+-------+\n",
      "|   0|     0|      0|\n",
      "+----+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Counting missing values for each column\n",
    "missing_values = date_df.select(\n",
    "    [count(when(col(c).isNull(), c)).alias(c) for c in date_df.columns]\n",
    ")\n",
    "missing_values.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f0a2be6-9f37-46a7-b359-ed23169e7b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original row count: 286\n",
      "\n",
      "Cleaned data:\n",
      "+----------+------------+-------+\n",
      "|date      |mmm_yy      |week_no|\n",
      "+----------+------------+-------+\n",
      "|2024-01-01|Jan-24      |1      |\n",
      "|2024-01-02|Jan-24      |1      |\n",
      "|2024-01-03|Jan-24      |1      |\n",
      "|2024-01-04|Jan-24      |1      |\n",
      "|2024-01-05|Jan-24      |1      |\n",
      "|2024-01-06|Jan-24      |1      |\n",
      "|2024-01-08|JAN-24      |2      |\n",
      "|2024-01-09|Jan-24      |2      |\n",
      "|2024-01-10|Jan-24      |2      |\n",
      "|2024-01-11|Jan-24      |2      |\n",
      "|2024-01-12|JAN-24      |2      |\n",
      "|2024-01-13|Jan-24      |2      |\n",
      "|2024-01-14|Jan-24      |2      |\n",
      "|2024-01-15|JAN-24      |3      |\n",
      "|2024-01-16|Jan-24      |3      |\n",
      "|2024-01-17|Jan-24      |3      |\n",
      "|2024-01-18|invalid_date|3      |\n",
      "|2024-01-19|Jan-24      |3      |\n",
      "|2024-01-20|Jan-24      |3      |\n",
      "|2024-01-21|JAN-24      |3      |\n",
      "+----------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "Summary of changes:\n",
      "Original row count: 286\n",
      "Final row count after cleaning: 286\n",
      "\n",
      "Schema of cleaned dataframe:\n",
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- mmm_yy: string (nullable = true)\n",
      " |-- week_no: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, regexp_extract\n",
    "\n",
    "\n",
    "\n",
    "# Show initial count\n",
    "print(\"Original row count:\", date_df.count())\n",
    "\n",
    "# 1. Drop null values and invalid_date entries\n",
    "date_df = date_df.na.drop(how='any') \\\n",
    "    .filter(~col(\"mmm_yy\").contains(\"invalid_date\"))\n",
    "\n",
    "# 2. Extract number from week_no (remove 'w' prefix)\n",
    "date_df = cleaned_date_df.withColumn(\n",
    "    \"week_no\",\n",
    "    regexp_extract(col(\"week_no\"), r\"(\\d+)\", 1)  # Extract only the number\n",
    ")\n",
    "\n",
    "# Show results\n",
    "print(\"\\nCleaned data:\")\n",
    "date_df.show(truncate=False)\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nSummary of changes:\")\n",
    "print(\"Original row count:\", date_df.count())\n",
    "print(\"Final row count after cleaning:\", date_df.count())\n",
    "\n",
    "# Show schema of cleaned dataframe\n",
    "print(\"\\nSchema of cleaned dataframe:\")\n",
    "date_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "efc2e778-57ab-49b3-bc96-74159e04ca06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original row count: 254\n",
      "\n",
      "Cleaned data:\n",
      "+----------+------+-------+\n",
      "|date      |mmm_yy|week_no|\n",
      "+----------+------+-------+\n",
      "|2024-01-01|Jan-24|1      |\n",
      "|2024-01-02|Jan-24|1      |\n",
      "|2024-01-03|Jan-24|1      |\n",
      "|2024-01-04|Jan-24|1      |\n",
      "|2024-01-05|Jan-24|1      |\n",
      "|2024-01-06|Jan-24|1      |\n",
      "|2024-01-08|JAN-24|2      |\n",
      "|2024-01-09|Jan-24|2      |\n",
      "|2024-01-10|Jan-24|2      |\n",
      "|2024-01-11|Jan-24|2      |\n",
      "|2024-01-12|JAN-24|2      |\n",
      "|2024-01-13|Jan-24|2      |\n",
      "|2024-01-14|Jan-24|2      |\n",
      "|2024-01-15|JAN-24|3      |\n",
      "|2024-01-16|Jan-24|3      |\n",
      "|2024-01-17|Jan-24|3      |\n",
      "|2024-01-19|Jan-24|3      |\n",
      "|2024-01-20|Jan-24|3      |\n",
      "|2024-01-21|JAN-24|3      |\n",
      "|2024-01-23|JAN-24|4      |\n",
      "+----------+------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "Summary of changes:\n",
      "Original row count: 254\n",
      "Final row count after cleaning: 254\n",
      "\n",
      "Checking for any remaining 'invalid_date' entries:\n",
      "+----+------+-------+\n",
      "|date|mmm_yy|week_no|\n",
      "+----+------+-------+\n",
      "+----+------+-------+\n",
      "\n",
      "\n",
      "Schema of cleaned dataframe:\n",
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- mmm_yy: string (nullable = true)\n",
      " |-- week_no: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, regexp_extract, lower\n",
    "\n",
    "# Metadata\n",
    "# Date: 2025-01-14 18:32:45 UTC\n",
    "# User: alexio545\n",
    "\n",
    "# Show initial count\n",
    "print(\"Original row count:\", date_df.count())\n",
    "\n",
    "# 1. Drop null values and invalid_date entries (case-insensitive)\n",
    "cleaned_date_df = date_df.na.drop(how='any') \\\n",
    "    .filter(\n",
    "        ~lower(col(\"mmm_yy\")).contains(\"invalid_date\") &  # Case-insensitive check\n",
    "        col(\"mmm_yy\").isNotNull()  # Additional null check\n",
    "    )\n",
    "\n",
    "# 2. Extract number from week_no (remove 'w' prefix)\n",
    "date_df = cleaned_date_df.withColumn(\n",
    "    \"week_no\",\n",
    "    regexp_extract(col(\"week_no\"), r\"(\\d+)\", 1)  # Extract only the number\n",
    ")\n",
    "\n",
    "# Show results with more rows to verify\n",
    "print(\"\\nCleaned data:\")\n",
    "cleaned_date_df.show(20, truncate=False)  # Showing more rows to verify\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nSummary of changes:\")\n",
    "print(\"Original row count:\", date_df.count())\n",
    "print(\"Final row count after cleaning:\", date_df.count())\n",
    "\n",
    "# Verify no invalid_date remains\n",
    "print(\"\\nChecking for any remaining 'invalid_date' entries:\")\n",
    "date_df.filter(lower(col(\"mmm_yy\")).contains(\"invalid_date\")).show()\n",
    "\n",
    "# Show schema of cleaned dataframe\n",
    "print(\"\\nSchema of cleaned dataframe:\")\n",
    "date_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3792d7ff-9498-4468-b44e-9cfd7d451691",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
