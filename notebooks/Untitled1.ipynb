{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "799a0e9e-9c36-4a18-a1fe-dac9c8005a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customers DataFrame Schema:\n",
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n",
      "Number of rows: 200\n",
      "+-----------+-------------+----+\n",
      "|customer_id|customer_name|city|\n",
      "+-----------+-------------+----+\n",
      "|          0|           22|  21|\n",
      "+-----------+-------------+----+\n",
      "\n",
      "25/01/14 19:36:32 WARN CacheManager: Asked to cache already cached data.\n",
      "+-----------+-------------+----+\n",
      "|customer_id|customer_name|city|\n",
      "+-----------+-------------+----+\n",
      "|          0|            0|   0|\n",
      "+-----------+-------------+----+\n",
      "\n",
      "Row count after transformation: 200\n",
      "root\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n",
      "+-----------+-----------------+-----------------+\n",
      "|customer_id|    customer_name|             city|\n",
      "+-----------+-----------------+-----------------+\n",
      "|     789221|      Info Stores|     Polokwane 11|\n",
      "|     789301|             null|           durban|\n",
      "|     789121|      Coolblue 96|        Cape Town|\n",
      "|     789501|     Logic Stores|port elizabeth-10|\n",
      "|     789201|         coolblue|   port elizabeth|\n",
      "|     789422|    viveks stores|             null|\n",
      "|     789603|        Rel Fresh|      East London|\n",
      "|     789201|        rel fresh|             null|\n",
      "|     789303|             null|INVALID_CITY20499|\n",
      "|     123.45|             null|INVALID_CITY55754|\n",
      "|     789520|       Lotus Mart|             null|\n",
      "|     789902|    Viveks Stores|      East London|\n",
      "|     789522|    Elite Mart_74|INVALID_CITY58916|\n",
      "|     789101|    Viveks Stores|          durban1|\n",
      "|         ID|     elite mart98|         Pretoria|\n",
      "|     789301|    viveks stores|     East London1|\n",
      "|     789420|Acclaimed Stores7|        Polokwane|\n",
      "|     789420|             null|        polokwane|\n",
      "|     789303|             null|INVALID_CITY65781|\n",
      "|     789122|       Elite Mart| port elizabeth10|\n",
      "+-----------+-----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----------+--------------------+-----------------+\n",
      "|customer_id|       customer_name|             city|\n",
      "+-----------+--------------------+-----------------+\n",
      "|  1000000.0|                null|        polokwane|\n",
      "|     -99999|         PROPEL MART|        Cape Town|\n",
      "|  1000000.0|                null|        Polokwane|\n",
      "|     -99999|        Sorefoz Mart|      pretoria 14|\n",
      "|  1000000.0|          Elite Mart|      East London|\n",
      "|  1000000.0|        logic stores|        kimberley|\n",
      "|     -99999|      propel mart_11|             null|\n",
      "|     -99999|         Expert Mart|        nelspruit|\n",
      "|     -99999|   Chiptec Stores_42|      cape town 9|\n",
      "|     -99999|       lotus mart 74|        Kimberley|\n",
      "|  1000000.0|        logic stores|        Nelspruit|\n",
      "|  1000000.0|            COOLBLUE|           durban|\n",
      "|     -99999|expression stores_62|        Polokwane|\n",
      "|  1000000.0|          ELITE MART|     Nelspruit 18|\n",
      "|     -99999|            COOLBLUE|INVALID_CITY62618|\n",
      "|     -99999|          Lotus Mart|        Cape Town|\n",
      "|     -99999|        Lotus Mart10|     east london8|\n",
      "|     -99999|         expert mart|        cape town|\n",
      "|  1000000.0|         info stores|      Nelspruit11|\n",
      "|     -99999|        INVALID_NAME|      pretoria 17|\n",
      "+-----------+--------------------+-----------------+\n",
      "\n",
      "25/01/14 19:36:32 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records processed: 200\n",
      "\n",
      "Schema of final dataframe:\n",
      "root\n",
      " |-- customer_id_hash: string (nullable = false)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n",
      "\n",
      "Sample of processed data:\n",
      "+--------------------------------+-------------+-----------------+\n",
      "|customer_id_hash                |customer_name|city             |\n",
      "+--------------------------------+-------------+-----------------+\n",
      "|1e7746579353fc48ce01dad8221156c9|Info Stores  |Polokwane 11     |\n",
      "|c270645522c203f91626a17998317cc8|null         |durban           |\n",
      "|b979555b640767d3f2603e29319024f0|Coolblue 96  |Cape Town        |\n",
      "|6d9be2294eb49598988c36e46e3d630a|Logic Stores |port elizabeth-10|\n",
      "|4ac660ea4ac050c24e503263f4e29107|coolblue     |port elizabeth   |\n",
      "+--------------------------------+-------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------+-----------------+--------------+\n",
      "|customer_id|customer_name    |city          |\n",
      "+-----------+-----------------+--------------+\n",
      "|789221     |Info Stores      |Polokwane     |\n",
      "|789301     |Unknown          |Durban        |\n",
      "|789121     |Coolblue 96      |Cape Town     |\n",
      "|789501     |Logic Stores     |Port Elizabeth|\n",
      "|789201     |coolblue         |Port Elizabeth|\n",
      "|789422     |viveks stores    |Unknown       |\n",
      "|789603     |Rel Fresh        |East London   |\n",
      "|1000000.0  |Unknown          |Polokwane     |\n",
      "|789201     |rel fresh        |Unknown       |\n",
      "|789303     |Unknown          |Unknown       |\n",
      "|123.45     |Unknown          |Unknown       |\n",
      "|789520     |Lotus Mart       |Unknown       |\n",
      "|789902     |Viveks Stores    |East London   |\n",
      "|789522     |Elite Mart_74    |Unknown       |\n",
      "|789101     |Viveks Stores    |durban1       |\n",
      "|ID         |elite mart98     |Pretoria      |\n",
      "|789301     |viveks stores    |East London1  |\n",
      "|789420     |Acclaimed Stores7|Polokwane     |\n",
      "|789420     |Unknown          |Polokwane     |\n",
      "|789303     |Unknown          |Unknown       |\n",
      "+-----------+-----------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Number of duplicate records found: 4\n",
      "\n",
      "Duplicate records found:\n",
      "+-----------+-------------+---------+-----+\n",
      "|customer_id|customer_name|city     |count|\n",
      "+-----------+-------------+---------+-----+\n",
      "|1000000.0  |Unknown      |Polokwane|2    |\n",
      "|123.45     |Unknown      |Unknown  |3    |\n",
      "|789303     |Unknown      |Unknown  |2    |\n",
      "|789122     |logic stores |Cape Town|2    |\n",
      "+-----------+-------------+---------+-----+\n",
      "\n",
      "\n",
      "Original record count: 200\n",
      "Record count after removing duplicates: 195\n",
      "\n",
      "Final cleaned and deduplicated data:\n",
      "+-----------+----------------+--------------+\n",
      "|customer_id|customer_name   |city          |\n",
      "+-----------+----------------+--------------+\n",
      "|1000000.0  |Unknown         |Polokwane     |\n",
      "|789420     |Acclaimed Stores|Unknown       |\n",
      "|abcd       |Propel Mart     |Nelspruit     |\n",
      "|789320     |Propel Mart_95  |Unknown       |\n",
      "|123.45     |Rel Fresh       |East London   |\n",
      "|789201     |Sorefoz Mart    |Durban        |\n",
      "|789102     |Lotus Mart      |Nelspruit     |\n",
      "|789503     |Unknown         |Bloemfontein  |\n",
      "|789121     |Elite Mart      |Nelspruit     |\n",
      "|789421     |Unknown         |Cape Town     |\n",
      "|789501     |vijay stores    |Polokwane     |\n",
      "|789521     |Viveks Stores   |Pretoria10    |\n",
      "|789401     |Unknown         |Pretoria      |\n",
      "|789420     |Unknown         |Port Elizabeth|\n",
      "|789422     |CHIPTEC STORES  |Unknown       |\n",
      "|789622     |chiptec stores  |Unknown       |\n",
      "|789121     |Propel Mart_80  |Kimberley     |\n",
      "|789421     |Viveks Stores_15|Unknown       |\n",
      "|789603     |expert mart     |Unknown       |\n",
      "|789603     |Vijay Stores    |East London   |\n",
      "+-----------+----------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Products DataFrame Schema:\n",
      "root\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      "\n",
      "Number of rows: 100\n",
      "25/01/14 19:36:35 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: product_name, product_id, category\n",
      " Schema: product_id, product_name, category\n",
      "Expected: product_id but found: product_name\n",
      "CSV file: file:///workspace/data/products.csv\n",
      "+----------+------------+--------+\n",
      "|product_id|product_name|category|\n",
      "+----------+------------+--------+\n",
      "|         0|           0|      16|\n",
      "+----------+------------+--------+\n",
      "\n",
      "25/01/14 19:36:35 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: product_name, product_id, category\n",
      " Schema: product_id, product_name, category\n",
      "Expected: product_id but found: product_name\n",
      "CSV file: file:///workspace/data/products.csv\n",
      "+----------+------------+--------+\n",
      "|product_id|product_name|category|\n",
      "+----------+------------+--------+\n",
      "|         0|           0|       0|\n",
      "+----------+------------+--------+\n",
      "\n",
      "+--------------------+------------+---------------+\n",
      "|          product_id|product_name|       category|\n",
      "+--------------------+------------+---------------+\n",
      "|    COCA COLA 500ML_|     1001001|      Beverages|\n",
      "|           Flour 2kg|     1005001|      Groceries|\n",
      "|     Coca Cola 500ml|     1001001|InvalidCategory|\n",
      "|         FLOUR 2KG70|     1005001|        Unknown|\n",
      "|INSTANT COFFEE 200G_|     1008002|      Beverages|\n",
      "+--------------------+------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Corrected column data:\n",
      "25/01/14 19:36:35 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: product_name, product_id, category\n",
      " Schema: product_id, product_name, category\n",
      "Expected: product_id but found: product_name\n",
      "CSV file: file:///workspace/data/products.csv\n",
      "+----------+--------------------+---------------+\n",
      "|product_id|product_name        |category       |\n",
      "+----------+--------------------+---------------+\n",
      "|1001001   |COCA COLA 500ML_    |Beverages      |\n",
      "|1005001   |Flour 2kg           |Groceries      |\n",
      "|1001001   |Coca Cola 500ml     |InvalidCategory|\n",
      "|1005001   |FLOUR 2KG70         |null           |\n",
      "|1008002   |INSTANT COFFEE 200G_|Beverages      |\n",
      "|-99999    |instant coffee 100g_|Beverages      |\n",
      "|1007001   |YOGURT PLAIN 500ML93|Dairy          |\n",
      "|1001003   |Sprite 500ml        |InvalidCategory|\n",
      "|1005002   |Flour 1kg           |Groceries      |\n",
      "|1005001   |FLOUR 2KG           |Groceries      |\n",
      "|-99999    |rice 5kg            |null           |\n",
      "|ID        |tea leaves 250g     |Beverages      |\n",
      "|1006001   |CHEDDAR CHEESE 250G |Dairy          |\n",
      "|1001003   |Sprite 500ml        |null           |\n",
      "|1003002   |Sugar 500g_         |Groceries      |\n",
      "|1005001   |Flour 2kg_          |InvalidCategory|\n",
      "|1004002   |RICE 1KG            |InvalidCategory|\n",
      "|1001003   |sprite 500ml_       |Beverages      |\n",
      "|1007002   |YOGURT PLAIN 250ML  |null           |\n",
      "|1005001   |Flour 2kg69         |Groceries      |\n",
      "+----------+--------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Cleaned and standardized products data:\n",
      "25/01/14 19:36:35 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: product_name, product_id, category\n",
      " Schema: product_id, product_name, category\n",
      "Expected: product_id but found: product_name\n",
      "CSV file: file:///workspace/data/products.csv\n",
      "+----------+-------------------+---------+\n",
      "|product_id|product_name       |category |\n",
      "+----------+-------------------+---------+\n",
      "|-99999    |Instant Coffee 100g|Beverages|\n",
      "|123.45    |Flour 2kg          |Groceries|\n",
      "|1001001   |Coca Cola 500ml    |Unknown  |\n",
      "|PROD2351  |Flour 1kg          |Groceries|\n",
      "|1008001   |Instant Coffee 100g|Beverages|\n",
      "|1007001   |Yogurt Plain 500ml |Dairy    |\n",
      "|1007002   |Yogurt Plain 250ml |Unknown  |\n",
      "|1006002   |Cheddar Cheese 500g|Unknown  |\n",
      "|1007001   |Yogurt Plain 500ml |Unknown  |\n",
      "|1006002   |Cheddar Cheese 500g|Dairy    |\n",
      "|1001003   |Sprite 500ml       |Beverages|\n",
      "|1005001   |Flour 2kg          |Groceries|\n",
      "|ID        |Yogurt Plain 250ml |Dairy    |\n",
      "|1004002   |Rice 1kg           |Unknown  |\n",
      "|ID        |Tea Leaves 250g    |Beverages|\n",
      "|1002001   |Cooking Oil 1l     |Unknown  |\n",
      "|-99999    |Cheddar Cheese 250g|Dairy    |\n",
      "|1009002   |Tea Leaves 500g    |Beverages|\n",
      "|1001002   |Pepsi 500ml        |Beverages|\n",
      "|1005002   |Flour 1kg          |Groceries|\n",
      "+----------+-------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "Summary of changes:\n",
      "Original row count: 100\n",
      "25/01/14 19:36:35 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: product_name, product_id, category\n",
      " Schema: product_id, product_name, category\n",
      "Expected: product_id but found: product_name\n",
      "CSV file: file:///workspace/data/products.csv\n",
      "Final row count: 54\n",
      "Products with hashed IDs:\n",
      "25/01/14 19:36:35 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: product_name, category\n",
      " Schema: product_id, category\n",
      "Expected: product_id but found: product_name\n",
      "CSV file: file:///workspace/data/products.csv\n",
      "+--------------------------------+-------------------+---------+\n",
      "|product_id                      |product_name       |category |\n",
      "+--------------------------------+-------------------+---------+\n",
      "|59e9f4bf700031a537a7495085e108c4|Cheddar Cheese 250g|Dairy    |\n",
      "|bd984327e9f06bf46e749d2e1300bb08|Cheddar Cheese 250g|Unknown  |\n",
      "|fde194048278a967cd1e31df11a23052|Cheddar Cheese 500g|Dairy    |\n",
      "|42687c68689652bb63145761b0120672|Cheddar Cheese 500g|Unknown  |\n",
      "|374765b4eb2766da24879be557999ad3|Coca Cola 500ml    |Beverages|\n",
      "|f90fd885f02cb1c92002c744b91dade5|Coca Cola 500ml    |Unknown  |\n",
      "|3c0594ca73dba30ca02ffb2bfadd45c1|Cooking Oil 1l     |Groceries|\n",
      "|15dd4eed8967ffc36ec15da1bb5ec094|Cooking Oil 1l     |Unknown  |\n",
      "|c4ce0dac2a59479a54fd2390ea3aff75|Cooking Oil 500ml  |Groceries|\n",
      "|cced1eca77baae1515d28b3551b2ba5e|Flour 1kg          |Groceries|\n",
      "|1efeef6291721b1b3cea0a8b4a93a1fb|Flour 1kg          |Unknown  |\n",
      "|bdbd13b42313c69b19f2f502f0cd773e|Flour 2kg          |Groceries|\n",
      "|a51df4c002dddadbb081b25c53e28595|Flour 2kg          |Unknown  |\n",
      "|5238265e0712dccef6a797b7ba95fc19|Instant Coffee 100g|Beverages|\n",
      "|f84a6998837a75b512e302618f45a15f|Instant Coffee 100g|Unknown  |\n",
      "|e4c705cd3c24eda69c4448d294a6e822|Instant Coffee 200g|Beverages|\n",
      "|6b28baacd380a0b0fe9a37d51bf14181|Instant Coffee 200g|Unknown  |\n",
      "|05f99abad0a9a5a773c88146e9ff1aee|Pepsi 500ml        |Beverages|\n",
      "|102bf0789865aa068f05948feeb143aa|Rice 1kg           |Groceries|\n",
      "|684e14267ff0d254f26aa3437c443c24|Rice 1kg           |Unknown  |\n",
      "+--------------------------------+-------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "Summary:\n",
      "Original row count: 100\n",
      "Final row count: 36\n",
      "\n",
      "Schema of final dataframe:\n",
      "root\n",
      " |-- product_id: string (nullable = false)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      "\n",
      "Dates DataFrame Schema:\n",
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- mmm_yy: string (nullable = true)\n",
      " |-- week_no: string (nullable = true)\n",
      "\n",
      "Number of rows: 366\n",
      "+----+------+-------+\n",
      "|date|mmm_yy|week_no|\n",
      "+----+------+-------+\n",
      "|  42|    41|      0|\n",
      "+----+------+-------+\n",
      "\n",
      "Original row count: 366\n",
      "\n",
      "Cleaned data (rows with no missing values):\n",
      "+----------+------------+-------+\n",
      "|date      |mmm_yy      |week_no|\n",
      "+----------+------------+-------+\n",
      "|2024-01-01|Jan-24      |W1     |\n",
      "|2024-01-02|Jan-24      |w1     |\n",
      "|2024-01-03|Jan-24      |W1     |\n",
      "|2024-01-04|Jan-24      |W1     |\n",
      "|2024-01-05|Jan-24      |W1     |\n",
      "|2024-01-06|Jan-24      |W1     |\n",
      "|2024-01-08|JAN-24      |W2     |\n",
      "|2024-01-09|Jan-24      |w2     |\n",
      "|2024-01-10|Jan-24      |W2     |\n",
      "|2024-01-11|Jan-24      |W2     |\n",
      "|2024-01-12|JAN-24      |W2     |\n",
      "|2024-01-13|Jan-24      |W2     |\n",
      "|2024-01-14|Jan-24      |W2     |\n",
      "|2024-01-15|JAN-24      |W3     |\n",
      "|2024-01-16|Jan-24      |w3     |\n",
      "|2024-01-17|Jan-24      |w3     |\n",
      "|2024-01-18|invalid_date|W3     |\n",
      "|2024-01-19|Jan-24      |W3     |\n",
      "|2024-01-20|Jan-24      |W3     |\n",
      "|2024-01-21|JAN-24      |w3     |\n",
      "+----------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "Schema of cleaned dataframe:\n",
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- mmm_yy: string (nullable = true)\n",
      " |-- week_no: string (nullable = true)\n",
      "\n",
      "+----+------+-------+\n",
      "|date|mmm_yy|week_no|\n",
      "+----+------+-------+\n",
      "|   0|     0|      0|\n",
      "+----+------+-------+\n",
      "\n",
      "\n",
      "Top 20 Customer Records:\n",
      "+-----------+----------------+--------------+\n",
      "|customer_id|customer_name   |city          |\n",
      "+-----------+----------------+--------------+\n",
      "|1000000.0  |Unknown         |Polokwane     |\n",
      "|789420     |Acclaimed Stores|Unknown       |\n",
      "|abcd       |Propel Mart     |Nelspruit     |\n",
      "|789320     |Propel Mart_95  |Unknown       |\n",
      "|123.45     |Rel Fresh       |East London   |\n",
      "|789201     |Sorefoz Mart    |Durban        |\n",
      "|789102     |Lotus Mart      |Nelspruit     |\n",
      "|789503     |Unknown         |Bloemfontein  |\n",
      "|789121     |Elite Mart      |Nelspruit     |\n",
      "|789421     |Unknown         |Cape Town     |\n",
      "|789501     |vijay stores    |Polokwane     |\n",
      "|789521     |Viveks Stores   |Pretoria10    |\n",
      "|789401     |Unknown         |Pretoria      |\n",
      "|789420     |Unknown         |Port Elizabeth|\n",
      "|789422     |CHIPTEC STORES  |Unknown       |\n",
      "|789622     |chiptec stores  |Unknown       |\n",
      "|789121     |Propel Mart_80  |Kimberley     |\n",
      "|789421     |Viveks Stores_15|Unknown       |\n",
      "|789603     |expert mart     |Unknown       |\n",
      "|789603     |Vijay Stores    |East London   |\n",
      "+-----------+----------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "Top 20 Product Records:\n",
      "+--------------------------------+-------------------+---------+\n",
      "|product_id                      |product_name       |category |\n",
      "+--------------------------------+-------------------+---------+\n",
      "|59e9f4bf700031a537a7495085e108c4|Cheddar Cheese 250g|Dairy    |\n",
      "|bd984327e9f06bf46e749d2e1300bb08|Cheddar Cheese 250g|Unknown  |\n",
      "|fde194048278a967cd1e31df11a23052|Cheddar Cheese 500g|Dairy    |\n",
      "|42687c68689652bb63145761b0120672|Cheddar Cheese 500g|Unknown  |\n",
      "|374765b4eb2766da24879be557999ad3|Coca Cola 500ml    |Beverages|\n",
      "|f90fd885f02cb1c92002c744b91dade5|Coca Cola 500ml    |Unknown  |\n",
      "|3c0594ca73dba30ca02ffb2bfadd45c1|Cooking Oil 1l     |Groceries|\n",
      "|15dd4eed8967ffc36ec15da1bb5ec094|Cooking Oil 1l     |Unknown  |\n",
      "|c4ce0dac2a59479a54fd2390ea3aff75|Cooking Oil 500ml  |Groceries|\n",
      "|cced1eca77baae1515d28b3551b2ba5e|Flour 1kg          |Groceries|\n",
      "|1efeef6291721b1b3cea0a8b4a93a1fb|Flour 1kg          |Unknown  |\n",
      "|bdbd13b42313c69b19f2f502f0cd773e|Flour 2kg          |Groceries|\n",
      "|a51df4c002dddadbb081b25c53e28595|Flour 2kg          |Unknown  |\n",
      "|5238265e0712dccef6a797b7ba95fc19|Instant Coffee 100g|Beverages|\n",
      "|f84a6998837a75b512e302618f45a15f|Instant Coffee 100g|Unknown  |\n",
      "|e4c705cd3c24eda69c4448d294a6e822|Instant Coffee 200g|Beverages|\n",
      "|6b28baacd380a0b0fe9a37d51bf14181|Instant Coffee 200g|Unknown  |\n",
      "|05f99abad0a9a5a773c88146e9ff1aee|Pepsi 500ml        |Beverages|\n",
      "|102bf0789865aa068f05948feeb143aa|Rice 1kg           |Groceries|\n",
      "|684e14267ff0d254f26aa3437c443c24|Rice 1kg           |Unknown  |\n",
      "+--------------------------------+-------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "Top 20 Date Records:\n",
      "+----------+------+-------+\n",
      "|date      |mmm_yy|week_no|\n",
      "+----------+------+-------+\n",
      "|2024-01-01|Jan-24|1      |\n",
      "|2024-01-02|Jan-24|1      |\n",
      "|2024-01-03|Jan-24|1      |\n",
      "|2024-01-04|Jan-24|1      |\n",
      "|2024-01-05|Jan-24|1      |\n",
      "|2024-01-06|Jan-24|1      |\n",
      "|2024-01-08|JAN-24|2      |\n",
      "|2024-01-09|Jan-24|2      |\n",
      "|2024-01-10|Jan-24|2      |\n",
      "|2024-01-11|Jan-24|2      |\n",
      "|2024-01-12|JAN-24|2      |\n",
      "|2024-01-13|Jan-24|2      |\n",
      "|2024-01-14|Jan-24|2      |\n",
      "|2024-01-15|JAN-24|3      |\n",
      "|2024-01-16|Jan-24|3      |\n",
      "|2024-01-17|Jan-24|3      |\n",
      "|2024-01-19|Jan-24|3      |\n",
      "|2024-01-20|Jan-24|3      |\n",
      "|2024-01-21|JAN-24|3      |\n",
      "|2024-01-23|JAN-24|4      |\n",
      "+----------+------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, lit\n",
    "from pyspark.sql.functions import col, regexp_replace, trim, when, regexp_extract\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, isnan, when, count, date_format, to_date, to_timestamp\n",
    "import traceback\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataProcessing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Customers Schema\n",
    "customers_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"customer_name\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "])  \n",
    "\n",
    "# Reading Customer CSV\n",
    "customers_df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(customers_schema) \\\n",
    "    .load(\"../data/customers.csv\")\n",
    "\n",
    "# Check schemas\n",
    "print(\"Customers DataFrame Schema:\")\n",
    "customers_df.printSchema()\n",
    "\n",
    "### Checking the number of rows in the data\n",
    "num_rows = customers_df.count()\n",
    "print(f\"Number of rows: {num_rows}\")\n",
    "\n",
    "### Check for missing values\n",
    "# Counting missing values for each column\n",
    "missing_values = customers_df.select(\n",
    "    [count(when(col(c).isNull(), c)).alias(c) for c in customers_df.columns]\n",
    ")\n",
    "missing_values.show()\n",
    "\n",
    "### Substitute with a Default Value\n",
    "def clean_customer_names(customers_df):\n",
    "    \"\"\"\n",
    "    Efficiently handle missing names in customer dataset using PySpark.\n",
    "    Using na.fill() is more performant than withColumn() for simple replacements.\n",
    "    \"\"\"\n",
    "    name_columns = [col for col in customers_df.columns \n",
    "                   if any(name_field in col.lower() \n",
    "                         for name_field in ['customer_name','city'])]\n",
    "    \n",
    "    fill_dict = {col: \"Unknown\" for col in name_columns}\n",
    "    cleaned_df = customers_df.na.fill(fill_dict)\n",
    "    cleaned_df = cleaned_df.cache()\n",
    "    return cleaned_df\n",
    "\n",
    "cleaned_customers_df = clean_customer_names(customers_df)\n",
    "\n",
    "# Counting missing values for each column\n",
    "missing_values = cleaned_customers_df.select(\n",
    "    [count(when(col(c).isNull(), c)).alias(c) for c in cleaned_customers_df.columns]\n",
    ")\n",
    "missing_values.show()\n",
    "\n",
    "def standardize_customer_id(df, column_name: str = \"customer_id\"):\n",
    "    try:\n",
    "        if df is None or column_name not in df.columns:\n",
    "            print(\"Invalid DataFrame or column name.\")\n",
    "            return None\n",
    "        \n",
    "        pattern = \"^[0-9]+$\"\n",
    "        standardized_df = df.withColumn(\n",
    "            column_name,\n",
    "            when(\n",
    "                col(column_name).rlike(pattern),\n",
    "                col(column_name).cast(LongType())\n",
    "            ).otherwise(None)\n",
    "        )\n",
    "        \n",
    "        row_count = standardized_df.count()\n",
    "        print(f\"Row count after transformation: {row_count}\")\n",
    "        standardized_df = standardized_df.repartition(200)\n",
    "        return standardized_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error in function: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "standardized_customers_df = standardize_customer_id(customers_df, column_name=\"customer_id\")\n",
    "standardized_customers_df.printSchema()\n",
    "\n",
    "def filter_invalid_customer_id(df, column_name: str = \"customer_id\", invalid_values: list = [-99999, 1e6]):\n",
    "    try:\n",
    "        if df is None or column_name not in df.columns:\n",
    "            print(\"Invalid DataFrame or column name.\")\n",
    "            return None\n",
    "        \n",
    "        invalid_values_set = set(invalid_values)\n",
    "        filter_condition = ~col(column_name).isin(invalid_values_set)\n",
    "        df_filtered = df.filter(filter_condition)\n",
    "        return df_filtered\n",
    "    except Exception as e:\n",
    "        print(f\"Error in function: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "valid_customers_df = filter_invalid_customer_id(customers_df, column_name=\"customer_id\", invalid_values=[-99999, 1e6])\n",
    "valid_customers_df.show()\n",
    "\n",
    "def show_invalid_customer_ids(df, column_name: str = \"customer_id\", invalid_values: list = [-99999, 1e6]):\n",
    "    try:\n",
    "        if df is None or column_name not in df.columns:\n",
    "            print(\"Invalid DataFrame or column name.\")\n",
    "            return None\n",
    "        \n",
    "        invalid_values_set = set(invalid_values)\n",
    "        filter_condition = col(column_name).isin(invalid_values_set)\n",
    "        df_invalid = df.filter(filter_condition)\n",
    "        return df_invalid\n",
    "    except Exception as e:\n",
    "        print(f\"Error in function: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "invalid_customer_ids_df = show_invalid_customer_ids(customers_df)\n",
    "invalid_customer_ids_df.show()\n",
    "\n",
    "# Hashing\n",
    "customers_df_opt = customers_df.select(\"customer_name\", \"city\")\n",
    "customers_with_hash = customers_df_opt.withColumn(\n",
    "    \"customer_id_hash\",\n",
    "    md5(concat(\n",
    "        coalesce(col(\"customer_name\"), lit(\"\")),\n",
    "        lit(\"_\"),\n",
    "        coalesce(col(\"city\"), lit(\"\"))\n",
    "    ))\n",
    ")\n",
    "\n",
    "customers_with_hash.cache()\n",
    "final_df = customers_with_hash.select(\n",
    "    \"customer_id_hash\",\n",
    "    \"customer_name\",\n",
    "    \"city\"\n",
    ")\n",
    "\n",
    "final_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"city\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .save(\"output_path\")\n",
    "\n",
    "print(\"Number of records processed:\", final_df.count())\n",
    "print(\"\\nSchema of final dataframe:\")\n",
    "final_df.printSchema()\n",
    "print(\"\\nSample of processed data:\")\n",
    "final_df.show(5, truncate=False)\n",
    "\n",
    "# Reference cities list\n",
    "cities = [\n",
    "    \"Johannesburg\", \"Cape Town\", \"Durban\", \"Pretoria\", \"Port Elizabeth\", \n",
    "    \"East London\", \"Bloemfontein\", \"Nelspruit\", \"Polokwane\", \"Kimberley\"\n",
    "]\n",
    "\n",
    "# Create a broadcast variable for efficient lookup\n",
    "cities_broadcast = spark.sparkContext.broadcast([city.lower() for city in cities])\n",
    "\n",
    "# Clean and standardize cities, handle null names and invalid cities\n",
    "cleaned_customers_df = customers_df.withColumn(\n",
    "    \"customer_name\",\n",
    "    when(col(\"customer_name\").isNull(), lit(\"Unknown\"))\n",
    "    .otherwise(col(\"customer_name\"))\n",
    ").withColumn(\n",
    "    \"city\",\n",
    "    when(\n",
    "        (upper(col(\"city\")).rlike(\"INVALID_CITY[0-9]*\")) |\n",
    "        (col(\"city\").isNull()),\n",
    "        lit(\"Unknown\")\n",
    "    ).otherwise(\n",
    "        when(\n",
    "            lower(regexp_replace(\n",
    "                regexp_replace(col(\"city\"), r\"[-_]\\d+$|[\\s]+\\d+$\", \"\"),\n",
    "                r\"\\s+\", \" \"\n",
    "            )).isin(cities_broadcast.value),\n",
    "            initcap(regexp_replace(col(\"city\"), r\"[-_]\\d+$|[\\s]+\\d+$\", \"\"))\n",
    "        ).otherwise(col(\"city\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "# Show results\n",
    "cleaned_customers_df.select(\"customer_id\", \"customer_name\", \"city\").show(truncate=False)\n",
    "\n",
    "# Checking for duplicates\n",
    "duplicate_check = cleaned_customers_df.groupBy(\"customer_id\", \"customer_name\", \"city\") \\\n",
    "    .agg(count(\"*\").alias(\"count\")) \\\n",
    "    .filter(col(\"count\") > 1)\n",
    "\n",
    "print(\"Number of duplicate records found:\", duplicate_check.count())\n",
    "\n",
    "if duplicate_check.count() > 0:\n",
    "    print(\"\\nDuplicate records found:\")\n",
    "    duplicate_check.show(truncate=False)\n",
    "\n",
    "deduped_customers_df = cleaned_customers_df.dropDuplicates([\"customer_id\", \"customer_name\", \"city\"])\n",
    "\n",
    "print(\"\\nOriginal record count:\", cleaned_customers_df.count())\n",
    "print(\"Record count after removing duplicates:\", deduped_customers_df.count())\n",
    "print(\"\\nFinal cleaned and deduplicated data:\")\n",
    "deduped_customers_df.select(\"customer_id\", \"customer_name\", \"city\").show(truncate=False)\n",
    "\n",
    "deduped_customers_df.createOrReplaceTempView(\"cleaned_customers\")\n",
    "\n",
    "# Products Schema\n",
    "products_schema = StructType([\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "])  \n",
    "\n",
    "# Reading Products CSV\n",
    "products_df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(products_schema) \\\n",
    "    .load(\"../data/products.csv\")\n",
    "\n",
    "print(\"Products DataFrame Schema:\")\n",
    "products_df.printSchema()\n",
    "\n",
    "num_rows = products_df.count()\n",
    "print(f\"Number of rows: {num_rows}\")\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = products_df.select(\n",
    "    [count(when(col(c).isNull(), c)).alias(c) for c in products_df.columns]\n",
    ")\n",
    "missing_values.show()\n",
    "\n",
    "def clean_product_names(products_df):\n",
    "    \"\"\"\n",
    "    Efficiently handle missing names in products dataset using PySpark.\n",
    "    \"\"\"\n",
    "    name_columns = [col for col in products_df.columns \n",
    "                   if any(name_field in col.lower() \n",
    "                         for name_field in ['product_name','category'])]\n",
    "    \n",
    "    fill_dict = {col: \"Unknown\" for col in name_columns}\n",
    "    cleaned_df = products_df.na.fill(fill_dict)\n",
    "    cleaned_df = cleaned_df.cache()\n",
    "    return cleaned_df\n",
    "\n",
    "cleaned_products_df = clean_product_names(products_df)\n",
    "\n",
    "missing_values = cleaned_products_df.select(\n",
    "    [count(when(col(c).isNull(), c)).alias(c) for c in cleaned_products_df.columns]\n",
    ")\n",
    "missing_values.show()\n",
    "cleaned_products_df.show(5)\n",
    "\n",
    "# Simple and efficient column renaming\n",
    "products_df = products_df.select(\n",
    "    col(\"product_name\").alias(\"product_id\"),\n",
    "    col(\"product_id\").alias(\"product_name\"),\n",
    "    col(\"category\")\n",
    ")\n",
    "\n",
    "print(\"Corrected column data:\")\n",
    "products_df.show(truncate=False)\n",
    "\n",
    "# Clean and standardize the dataset\n",
    "cleaned_products_df = products_df.select(\n",
    "    col(\"product_id\"),\n",
    "    initcap(\n",
    "        regexp_replace(\n",
    "            regexp_replace(\n",
    "                regexp_replace(\n",
    "                    col(\"product_name\"),\n",
    "                    r'[_\\-]|\\d+$',\n",
    "                    ''\n",
    "                ),\n",
    "                r'\\s+',\n",
    "                ' '\n",
    "            ),\n",
    "            r'\\s+$',\n",
    "            ''\n",
    "        )\n",
    "    ).alias(\"product_name\"),\n",
    "    \n",
    "    when(\n",
    "        (col(\"category\").isNull()) | \n",
    "        (col(\"category\") == \"InvalidCategory\"), \n",
    "        lit(\"Unknown\")\n",
    "    ).otherwise(col(\"category\")).alias(\"category\")\n",
    ")\n",
    "\n",
    "final_products_df = cleaned_products_df.dropDuplicates([\"product_id\", \"product_name\", \"category\"])\n",
    "\n",
    "print(\"Cleaned and standardized products data:\")\n",
    "final_products_df.show(truncate=False)\n",
    "\n",
    "print(\"\\nSummary of changes:\")\n",
    "print(\"Original row count:\", products_df.count())\n",
    "print(\"Final row count:\", final_products_df.count())\n",
    "\n",
    "# Create hashed product_id\n",
    "cleaned_products_df = products_df.select(\n",
    "    initcap(\n",
    "        regexp_replace(\n",
    "            regexp_replace(\n",
    "                regexp_replace(\n",
    "                    col(\"product_name\"),\n",
    "                    r'[_\\-]|\\d+$',\n",
    "                    ''\n",
    "                ),\n",
    "                r'\\s+',\n",
    "                ' '\n",
    "            ),\n",
    "            r'\\s+$',\n",
    "            ''\n",
    "        )\n",
    "    ).alias(\"product_name\"),\n",
    "    \n",
    "    when(\n",
    "        (col(\"category\").isNull()) | \n",
    "        (col(\"category\") == \"InvalidCategory\"), \n",
    "        lit(\"Unknown\")\n",
    "    ).otherwise(col(\"category\")).alias(\"category\")\n",
    ")\n",
    "\n",
    "final_products_df = cleaned_products_df.withColumn(\n",
    "    \"product_id\",\n",
    "    md5(concat(\n",
    "        coalesce(col(\"product_name\"), lit(\"\")),\n",
    "        lit(\"_\"),\n",
    "        coalesce(col(\"category\"), lit(\"\"))\n",
    "    ))\n",
    ").select(\n",
    "    \"product_id\",\n",
    "    \"product_name\",\n",
    "    \"category\"\n",
    ")\n",
    "\n",
    "final_products_df.cache()\n",
    "final_products_df = final_products_df.dropDuplicates([\"product_name\", \"category\"])\n",
    "\n",
    "print(\"Products with hashed IDs:\")\n",
    "final_products_df.show(truncate=False)\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "print(\"Original row count:\", products_df.count())\n",
    "print(\"Final row count:\", final_products_df.count())\n",
    "print(\"\\nSchema of final dataframe:\")\n",
    "final_products_df.printSchema()\n",
    "\n",
    "# Dates Schema\n",
    "date_schema = StructType([\n",
    "    StructField(\"date\", DateType(), True),\n",
    "    StructField(\"mmm_yy\", StringType(), True),\n",
    "    StructField(\"week_no\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Reading Dates CSV\n",
    "date_df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(date_schema) \\\n",
    "    .load(\"../data/dates.csv\")\n",
    "\n",
    "print(\"Dates DataFrame Schema:\")\n",
    "date_df.printSchema()\n",
    "\n",
    "num_rows = date_df.count()\n",
    "print(f\"Number of rows: {num_rows}\")\n",
    "\n",
    "missing_values = date_df.select(\n",
    "    [count(when(col(c).isNull(), c)).alias(c) for c in date_df.columns]\n",
    ")\n",
    "missing_values.show()\n",
    "\n",
    "print(\"Original row count:\", date_df.count())\n",
    "date_df = date_df.na.drop(how='any')\n",
    "\n",
    "print(\"\\nCleaned data (rows with no missing values):\")\n",
    "date_df.show(truncate=False)\n",
    "\n",
    "print(\"\\nSchema of cleaned dataframe:\")\n",
    "date_df.printSchema()\n",
    "\n",
    "missing_values = date_df.select(\n",
    "    [count(when(col(c).isNull(), c)).alias(c) for c in date_df.columns]\n",
    ")\n",
    "missing_values.show()\n",
    "\n",
    "# Clean dates data\n",
    "cleaned_date_df = date_df.na.drop(how='any') \\\n",
    "    .filter(\n",
    "        ~lower(col(\"mmm_yy\")).contains(\"invalid_date\") &\n",
    "        col(\"mmm_yy\").isNotNull()\n",
    "    ).withColumn(\n",
    "        \"week_no\",\n",
    "        regexp_extract(col(\"week_no\"), r\"(\\d+)\", 1)\n",
    "    )\n",
    "\n",
    "# Save final processed datasets\n",
    "deduped_customers_df.write.mode(\"overwrite\").parquet(\"output/customers\")\n",
    "final_products_df.write.mode(\"overwrite\").parquet(\"output/products\")\n",
    "cleaned_date_df.write.mode(\"overwrite\").parquet(\"output/dates\")\n",
    "\n",
    "# Show top 20 records from each dataset\n",
    "print(\"\\nTop 20 Customer Records:\")\n",
    "deduped_customers_df.show(20, truncate=False)\n",
    "\n",
    "print(\"\\nTop 20 Product Records:\")\n",
    "final_products_df.show(20, truncate=False)\n",
    "\n",
    "print(\"\\nTop 20 Date Records:\")\n",
    "cleaned_date_df.show(20, truncate=False)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7568c1f-198b-45f5-b489-0f7d22362119",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import md5, concat, col, lit, coalesce\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import regexp_replace, initcap, when, col, lower,upper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fac4e9c-503c-458b-bd52-fec52b03cd0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
