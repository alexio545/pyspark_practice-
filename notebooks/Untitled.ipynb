{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "671b841a-7430-48ac-a893-a55ebf95d550",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, when, lit, regexp_replace, trim, \n",
    "    regexp_extract, isnan, count, date_format,\n",
    "    to_date, to_timestamp\n",
    ")\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def create_spark_session(app_name=\"DataProcessing\"):\n",
    "    \"\"\"\n",
    "    Create a SparkSession.\n",
    "\n",
    "    Args:\n",
    "        app_name (str): The name of the Spark application.\n",
    "\n",
    "    Returns:\n",
    "        SparkSession: A Spark session object.\n",
    "    \"\"\"\n",
    "    return SparkSession.builder.appName(app_name).getOrCreate()\n",
    "\n",
    "def read_csv_with_schema(spark, file_path, schema):\n",
    "    \"\"\"\n",
    "    Read a CSV file with a specified schema.\n",
    "\n",
    "    Args:\n",
    "        spark (SparkSession): The Spark session.\n",
    "        file_path (str): Path to the CSV file.\n",
    "        schema (StructType): Schema to apply to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame containing the data.\n",
    "    \"\"\"\n",
    "    return spark.read.format(\"csv\").option(\"header\", True).schema(schema).load(file_path)\n",
    "\n",
    "def check_missing_values(df):\n",
    "    \"\"\"\n",
    "    Check for missing values in each column.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame showing the count of missing values for each column.\n",
    "    \"\"\"\n",
    "    return df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "\n",
    "def handle_missing_values(df, strategy=\"drop\"):\n",
    "    \"\"\"\n",
    "    Handle missing values in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input DataFrame.\n",
    "        strategy (str): Strategy to handle missing values ('drop' or 'retain').\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The DataFrame after handling missing values.\n",
    "    \"\"\"\n",
    "    if strategy == \"drop\":\n",
    "        return df.dropna()\n",
    "    return df\n",
    "\n",
    "def check_duplicates(df, id_column):\n",
    "    \"\"\"\n",
    "    Check for duplicate rows based on a specific column.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input DataFrame.\n",
    "        id_column (str): The column used to identify duplicates.\n",
    "\n",
    "    Returns:\n",
    "        int: The count of duplicate rows.\n",
    "    \"\"\"\n",
    "    duplicate_count = df.count() - df.dropDuplicates([id_column]).count()\n",
    "    return duplicate_count\n",
    "\n",
    "def transform_datetime_columns(df, timestamp_columns):\n",
    "    \"\"\"\n",
    "    Split timestamp columns into date and time components.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input DataFrame.\n",
    "        timestamp_columns (list): List of column names containing timestamps.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The DataFrame with split date and time components.\n",
    "    \"\"\"\n",
    "    for col_name in timestamp_columns:\n",
    "        df = (df\n",
    "            .withColumn(f\"date_{col_name}\", to_date(col(col_name), \"yyyy-MM-dd\"))\n",
    "            .withColumn(f\"time_{col_name}\", date_format(col(col_name), \"HH:mm:ss\"))\n",
    "        )\n",
    "    return df\n",
    "\n",
    "def clean_price_column(df):\n",
    "    \"\"\"\n",
    "    Clean and validate the price column.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The DataFrame with a cleaned and validated price column.\n",
    "    \"\"\"\n",
    "    return (df\n",
    "        .withColumn(\"price\", \n",
    "            regexp_extract(\n",
    "                regexp_replace(trim(col(\"price\")), \"^\\\\$\", \"\"),\n",
    "                \"^\\\\d+(\\\\.\\\\d+)?\", 0\n",
    "            )\n",
    "        )\n",
    "        .withColumn(\"price\", \n",
    "            when(\n",
    "                col(\"price\").rlike(\"^\\\\d+(\\\\.\\\\d+)?$\"),\n",
    "                col(\"price\").cast(\"double\")\n",
    "            ).otherwise(None)\n",
    "        )\n",
    "        .filter(col(\"price\").isNotNull())\n",
    "        .filter((col(\"price\") > 0) & (col(\"price\") <= 10000))\n",
    "    )\n",
    "\n",
    "def validate_listings(df):\n",
    "    \"\"\"\n",
    "    Validate the listings based on minimum nights.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The DataFrame after filtering valid listings.\n",
    "    \"\"\"\n",
    "    return df.filter((col(\"minimum_nights\") > 0) & (col(\"minimum_nights\") <= 365))\n",
    "\n",
    "def convert_superhost_to_boolean(df):\n",
    "    \"\"\"\n",
    "    Convert the 'is_superhost' column to boolean.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The DataFrame with a boolean 'is_superhost' column.\n",
    "    \"\"\"\n",
    "    return df.withColumn(\n",
    "        \"is_superhost\",\n",
    "        when(col(\"is_superhost\") == \"t\", True)\n",
    "        .when(col(\"is_superhost\") == \"f\", False)\n",
    "        .otherwise(None)\n",
    "    )\n",
    "\n",
    "def impute_text_columns(df, columns, default_value=\"Unknown\"):\n",
    "    \"\"\"\n",
    "    Impute missing text columns with a default value.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input DataFrame.\n",
    "        columns (list): List of column names to impute.\n",
    "        default_value (str): The default value for missing text.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The DataFrame with imputed text columns.\n",
    "    \"\"\"\n",
    "    for column in columns:\n",
    "        df = df.withColumn(\n",
    "            column,\n",
    "            when(col(column).isNull() | (col(column) == \"\"), lit(default_value))\n",
    "            .otherwise(col(column))\n",
    "        )\n",
    "    return df\n",
    "\n",
    "def process_hosts_data(spark, file_path):\n",
    "    \"\"\"\n",
    "    Process host data from a CSV file.\n",
    "\n",
    "    Args:\n",
    "        spark (SparkSession): The Spark session.\n",
    "        file_path (str): Path to the hosts CSV file.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The processed host data.\n",
    "    \"\"\"\n",
    "    hosts_schema = StructType([\n",
    "        StructField(\"id\", StringType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"is_superhost\", StringType(), True),\n",
    "        StructField(\"created_at\", TimestampType(), True),\n",
    "        StructField(\"updated_at\", TimestampType(), True)\n",
    "    ])\n",
    "    \n",
    "    df = (read_csv_with_schema(spark, file_path, hosts_schema)\n",
    "          .transform(lambda df: handle_missing_values(df))\n",
    "          .transform(convert_superhost_to_boolean)\n",
    "          .transform(lambda df: transform_datetime_columns(df, [\"created_at\", \"updated_at\"])))\n",
    "    return df\n",
    "\n",
    "def process_listings_data(spark, file_path):\n",
    "    \"\"\"\n",
    "    Process listing data from a CSV file.\n",
    "\n",
    "    Args:\n",
    "        spark (SparkSession): The Spark session.\n",
    "        file_path (str): Path to the listings CSV file.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The processed listing data.\n",
    "    \"\"\"\n",
    "    listings_schema = StructType([\n",
    "        StructField(\"id\", StringType(), True),\n",
    "        StructField(\"listing_url\", StringType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"room_type\", StringType(), True),\n",
    "        StructField(\"minimum_nights\", IntegerType(), True),\n",
    "        StructField(\"host_id\", StringType(), True),\n",
    "        StructField(\"price\", StringType(), True),\n",
    "        StructField(\"created_at\", TimestampType(), True),\n",
    "        StructField(\"updated_at\", TimestampType(), True)\n",
    "    ])\n",
    "    \n",
    "    df = (read_csv_with_schema(spark, file_path, listings_schema)\n",
    "          .transform(lambda df: handle_missing_values(df))\n",
    "          .transform(clean_price_column)\n",
    "          .transform(validate_listings)\n",
    "          .transform(lambda df: transform_datetime_columns(df, [\"created_at\", \"updated_at\"])))\n",
    "    return df\n",
    "\n",
    "def process_reviews_data(spark, file_path):\n",
    "    \"\"\"\n",
    "    Process review data from a CSV file.\n",
    "\n",
    "    Args:\n",
    "        spark (SparkSession): The Spark session.\n",
    "        file_path (str): Path to the reviews CSV file.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The processed review data.\n",
    "    \"\"\"\n",
    "    reviews_schema = StructType([\n",
    "        StructField(\"listing_id\", StringType(), True),\n",
    "        StructField(\"date\", TimestampType(), True),\n",
    "        StructField(\"reviewer_name\", StringType(), True),\n",
    "        StructField(\"comments\", StringType(), True),\n",
    "        StructField(\"sentiment\", StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    df = (read_csv_with_schema(spark, file_path, reviews_schema)\n",
    "          .transform(lambda df: impute_text_columns(df, [\"comments\", \"sentiment\"]))\n",
    "          .transform(lambda df: transform_datetime_columns(df, [\"date\"])))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f4ce83d-29a5-4cfe-aedc-dab800ce8e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/01/14 11:15:38 WARN Utils: Your hostname, codespaces-240eb1 resolves to a loopback address: 127.0.0.1; using 10.0.1.51 instead (on interface eth0)\n",
      "25/01/14 11:15:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/01/14 11:15:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.ExceptionInInitializerError\n\tat org.apache.spark.unsafe.array.ByteArrayMethods.<clinit>(ByteArrayMethods.java:56)\n\tat org.apache.spark.memory.MemoryManager.defaultPageSizeBytes$lzycompute(MemoryManager.scala:264)\n\tat org.apache.spark.memory.MemoryManager.defaultPageSizeBytes(MemoryManager.scala:254)\n\tat org.apache.spark.memory.MemoryManager.$anonfun$pageSizeBytes$1(MemoryManager.scala:273)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.memory.MemoryManager.<init>(MemoryManager.scala:273)\n\tat org.apache.spark.memory.UnifiedMemoryManager.<init>(UnifiedMemoryManager.scala:58)\n\tat org.apache.spark.memory.UnifiedMemoryManager$.apply(UnifiedMemoryManager.scala:207)\n\tat org.apache.spark.SparkEnv$.create(SparkEnv.scala:320)\n\tat org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:194)\n\tat org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:279)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:464)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: java.lang.IllegalStateException: java.lang.NoSuchMethodException: java.nio.DirectByteBuffer.<init>(long,int)\n\tat org.apache.spark.unsafe.Platform.<clinit>(Platform.java:113)\n\t... 25 more\nCaused by: java.lang.NoSuchMethodException: java.nio.DirectByteBuffer.<init>(long,int)\n\tat java.base/java.lang.Class.getConstructor0(Class.java:3761)\n\tat java.base/java.lang.Class.getDeclaredConstructor(Class.java:2930)\n\tat org.apache.spark.unsafe.Platform.<clinit>(Platform.java:71)\n\t... 25 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_spark_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m hosts_df \u001b[38;5;241m=\u001b[39m process_hosts_data(spark, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/hosts.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m listings_df \u001b[38;5;241m=\u001b[39m process_listings_data(spark, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/listings.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 19\u001b[0m, in \u001b[0;36mcreate_spark_session\u001b[0;34m(app_name)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate_spark_session\u001b[39m(app_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataProcessing\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     10\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    Create a SparkSession.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m        SparkSession: A Spark session object.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapp_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/pyspark_practice-/venv/lib/python3.12/site-packages/pyspark/sql/session.py:269\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    272\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[0;32m/workspaces/pyspark_practice-/venv/lib/python3.12/site-packages/pyspark/context.py:483\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 483\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m/workspaces/pyspark_practice-/venv/lib/python3.12/site-packages/pyspark/context.py:197\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[1;32m    195\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mappName\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparkHome\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpyFiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjsc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mudf_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[0;32m/workspaces/pyspark_practice-/venv/lib/python3.12/site-packages/pyspark/context.py:282\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    281\u001b[0m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc \u001b[38;5;241m=\u001b[39m jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf \u001b[38;5;241m=\u001b[39m SparkConf(_jconf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39mconf())\n",
      "File \u001b[0;32m/workspaces/pyspark_practice-/venv/lib/python3.12/site-packages/pyspark/context.py:402\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;124;03mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mJavaSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/pyspark_practice-/venv/lib/python3.12/site-packages/py4j/java_gateway.py:1585\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1579\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1580\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1581\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1582\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1584\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1585\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1588\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1589\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/workspaces/pyspark_practice-/venv/lib/python3.12/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.ExceptionInInitializerError\n\tat org.apache.spark.unsafe.array.ByteArrayMethods.<clinit>(ByteArrayMethods.java:56)\n\tat org.apache.spark.memory.MemoryManager.defaultPageSizeBytes$lzycompute(MemoryManager.scala:264)\n\tat org.apache.spark.memory.MemoryManager.defaultPageSizeBytes(MemoryManager.scala:254)\n\tat org.apache.spark.memory.MemoryManager.$anonfun$pageSizeBytes$1(MemoryManager.scala:273)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.memory.MemoryManager.<init>(MemoryManager.scala:273)\n\tat org.apache.spark.memory.UnifiedMemoryManager.<init>(UnifiedMemoryManager.scala:58)\n\tat org.apache.spark.memory.UnifiedMemoryManager$.apply(UnifiedMemoryManager.scala:207)\n\tat org.apache.spark.SparkEnv$.create(SparkEnv.scala:320)\n\tat org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:194)\n\tat org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:279)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:464)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: java.lang.IllegalStateException: java.lang.NoSuchMethodException: java.nio.DirectByteBuffer.<init>(long,int)\n\tat org.apache.spark.unsafe.Platform.<clinit>(Platform.java:113)\n\t... 25 more\nCaused by: java.lang.NoSuchMethodException: java.nio.DirectByteBuffer.<init>(long,int)\n\tat java.base/java.lang.Class.getConstructor0(Class.java:3761)\n\tat java.base/java.lang.Class.getDeclaredConstructor(Class.java:2930)\n\tat org.apache.spark.unsafe.Platform.<clinit>(Platform.java:71)\n\t... 25 more\n"
     ]
    }
   ],
   "source": [
    "spark = create_spark_session()\n",
    "hosts_df = process_hosts_data(spark, \"../data/hosts.csv\")\n",
    "listings_df = process_listings_data(spark, \"../data/listings.csv\")\n",
    "reviews_df = process_reviews_data(spark, \"../data/reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e862c846-068d-424c-8fff-4229b7a32b16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
