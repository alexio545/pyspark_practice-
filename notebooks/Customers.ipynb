{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9310abaa-7af1-4b31-8666-de87ca752e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, lit\n",
    "from pyspark.sql.functions import col, regexp_replace, trim, when, regexp_extract\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, isnan, when, count ,date_format,to_date,to_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cb23010-0dae-48c4-a2ea-49b0cd6caffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/01/14 13:22:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataProcessing\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab6ff444-347c-4f29-a2a7-214364a0ebd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://9fab1b8197c1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>DataProcessing</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x731b5dc198e0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ba51602-878d-43bd-ba21-e8752da9eafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Customers Schema\n",
    "customers_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"customer_name\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13dadbb3-a4aa-404e-b8e4-cc2c330ac040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Customer CSV\n",
    "customers_df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(customers_schema) \\\n",
    "    .load(\"../data/customers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16144f83-9bd0-407a-b19c-a5a0e8a66f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customers DataFrame Schema:\n",
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check schemas\n",
    "print(\"Customers DataFrame Schema:\")\n",
    "customers_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838ede7f-21ac-46ce-b0be-a683edf3d17f",
   "metadata": {},
   "source": [
    "###  Checkin the number of rows in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a488a4d-1cb1-413c-b324-1ce03d2226cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 200\n"
     ]
    }
   ],
   "source": [
    "num_rows = customers_df.count()\n",
    "print(f\"Number of rows: {num_rows}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da925166-7b04-42da-8d16-30b82d395cbb",
   "metadata": {},
   "source": [
    "### Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a25b5b63-fccf-477c-9d83-28ce55b05e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----+\n",
      "|customer_id|customer_name|city|\n",
      "+-----------+-------------+----+\n",
      "|          0|           22|  21|\n",
      "+-----------+-------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Counting missing values for each column\n",
    "missing_values = customers_df.select(\n",
    "    [count(when(col(c).isNull(), c)).alias(c) for c in customers_df.columns]\n",
    ")\n",
    "missing_values.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4655c3e9-d1e7-4995-a540-bfe0aa9cc277",
   "metadata": {},
   "source": [
    "### Substitute with a Default Value \n",
    "\n",
    "For the missing values in Customers , default values will be used eg Unknown "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d394fec9-3bb6-4b3f-a935-5a92e1e4b485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/01/14 15:13:01 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def clean_customer_names(customers_df):\n",
    "    \"\"\"\n",
    "    Efficiently handle missing names in customer dataset using PySpark.\n",
    "    Using na.fill() is more performant than withColumn() for simple replacements.\n",
    "    \n",
    "    Args:\n",
    "        customers_df: PySpark DataFrame containing customer data\n",
    "        \n",
    "    Returns:\n",
    "        PySpark DataFrame with missing names replaced with \"Unknown\"\n",
    "    \"\"\"\n",
    "    # Get list of name columns (assuming they might be first_name, last_name, or name)\n",
    "    name_columns = [col for col in customers_df.columns \n",
    "                   if any(name_field in col.lower() \n",
    "                         for name_field in ['customer_name','city'])]\n",
    "    \n",
    "    # Create dictionary of columns to fill\n",
    "    fill_dict = {col: \"Unknown\" for col in name_columns}\n",
    "    \n",
    "    # Use na.fill() which is more efficient than withColumn() for simple replacements\n",
    "    cleaned_df = customers_df.na.fill(fill_dict)\n",
    "    \n",
    "    # Cache the result if you'll be using it multiple times\n",
    "    cleaned_df = cleaned_df.cache()  # Uncomment if needed\n",
    "    \n",
    "    return cleaned_df\n",
    "\n",
    "\n",
    "# Example usage\n",
    "cleaned_customers_df = clean_customer_names(customers_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7d38e231-300a-4c37-bd74-71d44ccbd696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----+\n",
      "|customer_id|customer_name|city|\n",
      "+-----------+-------------+----+\n",
      "|          0|            0|   0|\n",
      "+-----------+-------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Counting missing values for each column\n",
    "missing_values = cleaned_customers_df.select(\n",
    "    [count(when(col(c).isNull(), c)).alias(c) for c in cleaned_customers_df.columns]\n",
    ")\n",
    "missing_values.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96621456-d559-4151-bb73-1ebaa31f23ed",
   "metadata": {},
   "source": [
    "### Standardizing  Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "008822d6-4103-405a-9ba8-7bb9b9e3bf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import LongType\n",
    "import traceback\n",
    "\n",
    "def standardize_customer_id(df, column_name: str = \"customer_id\"):\n",
    "    ''' Standardizes the customer_id column by converting valid numeric strings to LongType \n",
    "    and invalid values to None. The function also optimizes memory usage, minimizes shuffle \n",
    "    operations, and adapts partitioning based on data size.'''\n",
    "    try:\n",
    "        if df is None or column_name not in df.columns:\n",
    "            print(\"Invalid DataFrame or column name.\")\n",
    "            return None\n",
    "        \n",
    "        # Regex pattern for numeric values\n",
    "        pattern = \"^[0-9]+$\"\n",
    "        \n",
    "        # Apply transformation\n",
    "        standardized_df = df.withColumn(\n",
    "            column_name,\n",
    "            F.when(\n",
    "                F.col(column_name).rlike(pattern),\n",
    "                F.col(column_name).cast(LongType())\n",
    "            ).otherwise(None)\n",
    "        )\n",
    "        \n",
    "        # Log row count\n",
    "        row_count = standardized_df.count()\n",
    "        print(f\"Row count after transformation: {row_count}\")\n",
    "        \n",
    "        # Repartition if needed (optional for testing)\n",
    "        standardized_df = standardized_df.repartition(200)\n",
    "        \n",
    "        return standardized_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error in function: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "327c77ca-8c42-4b85-897e-b90a9f41f681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count after transformation: 200\n"
     ]
    }
   ],
   "source": [
    "standardized_customers_df = standardize_customer_id(customers_df, column_name=\"customer_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a71c3ee-d14e-4c8a-b2d3-8629b8815f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Verifying the schema\n",
    "standardized_customers_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f6d3af23-f525-406c-ba6a-329e791ae8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(type(customers_df))\n",
    "customers_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "03fac70b-03e6-423f-9266-851f37a3d6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import LongType, IntegerType\n",
    "import traceback\n",
    "\n",
    "def filter_invalid_customer_id(df, column_name: str = \"customer_id\", invalid_values: list = [-99999, 1e6]):\n",
    "    \"\"\"\n",
    "    Filters out rows where the customer_id column has nonsensical values like -99999 or 1e6.\n",
    "    This function uses efficient filtering methods optimized for computational cost and memory usage.\n",
    "\n",
    "    Args:\n",
    "        df (pyspark.sql.DataFrame): The input PySpark DataFrame containing the customer_id column.\n",
    "        column_name (str, optional): The name of the column to check for invalid values (default is \"customer_id\").\n",
    "        invalid_values (list, optional): List of values that should be considered invalid and filtered out (default is [-99999, 1e6]).\n",
    "\n",
    "    Returns:\n",
    "        pyspark.sql.DataFrame:\n",
    "            A new DataFrame with rows that do not contain invalid customer_id values. \n",
    "            Returns None if the input DataFrame is invalid or an error occurs.\n",
    "\n",
    "    Example:\n",
    "        >>> valid_customers_df = filter_invalid_customer_id(customers_df, column_name=\"customer_id\")\n",
    "        >>> valid_customers_df.show()\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if df is None or column_name not in df.columns:\n",
    "            print(\"Invalid DataFrame or column name.\")\n",
    "            return None\n",
    "        \n",
    "        # Convert invalid_values list to a set for faster lookup\n",
    "        invalid_values_set = set(invalid_values)\n",
    "        \n",
    "        # Create the filter condition to exclude rows with invalid customer_id values\n",
    "        filter_condition = ~F.col(column_name).isin(invalid_values_set)\n",
    "        \n",
    "        # Apply the filter and return the DataFrame with valid customer_id values\n",
    "        df_filtered = df.filter(filter_condition)\n",
    "\n",
    "        return df_filtered\n",
    "    except Exception as e:\n",
    "        print(f\"Error in function: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9fc96e9e-2be1-4f37-9419-2a2e97467d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+-----------------+\n",
      "|customer_id|    customer_name|             city|\n",
      "+-----------+-----------------+-----------------+\n",
      "|     789221|      Info Stores|     Polokwane 11|\n",
      "|     789301|             null|           durban|\n",
      "|     789121|      Coolblue 96|        Cape Town|\n",
      "|     789501|     Logic Stores|port elizabeth-10|\n",
      "|     789201|         coolblue|   port elizabeth|\n",
      "|     789422|    viveks stores|             null|\n",
      "|     789603|        Rel Fresh|      East London|\n",
      "|     789201|        rel fresh|             null|\n",
      "|     789303|             null|INVALID_CITY20499|\n",
      "|     123.45|             null|INVALID_CITY55754|\n",
      "|     789520|       Lotus Mart|             null|\n",
      "|     789902|    Viveks Stores|      East London|\n",
      "|     789522|    Elite Mart_74|INVALID_CITY58916|\n",
      "|     789101|    Viveks Stores|          durban1|\n",
      "|         ID|     elite mart98|         Pretoria|\n",
      "|     789301|    viveks stores|     East London1|\n",
      "|     789420|Acclaimed Stores7|        Polokwane|\n",
      "|     789420|             null|        polokwane|\n",
      "|     789303|             null|INVALID_CITY65781|\n",
      "|     789122|       Elite Mart| port elizabeth10|\n",
      "+-----------+-----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming customers_df is your original DataFrame\n",
    "valid_customers_df = filter_invalid_customer_id(customers_df, column_name=\"customer_id\", invalid_values=[-99999, 1e6])\n",
    "\n",
    "# Verifying the result\n",
    "valid_customers_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ae62638c-e316-40d6-98de-c0e0d3b170af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import traceback\n",
    "\n",
    "def show_invalid_customer_ids(df, column_name: str = \"customer_id\", invalid_values: list = [-99999, 1e6]):\n",
    "    \"\"\"\n",
    "    Displays all rows where customer_id has nonsensical values such as -99999 or 1e6.\n",
    "    \n",
    "    This function identifies rows with invalid customer_id values for inspection without making any changes to the data.\n",
    "\n",
    "    Args:\n",
    "        df (pyspark.sql.DataFrame): The input PySpark DataFrame containing the customer_id column.\n",
    "        column_name (str, optional): The name of the column to check for invalid values (default is \"customer_id\").\n",
    "        invalid_values (list, optional): List of invalid customer_id values (default is [-99999, 1e6]).\n",
    "\n",
    "    Returns:\n",
    "        pyspark.sql.DataFrame:\n",
    "            A DataFrame containing the rows with invalid customer_id values.\n",
    "            Returns None if the input DataFrame is invalid or an error occurs.\n",
    "\n",
    "    Example:\n",
    "        >>> invalid_customer_ids_df = show_invalid_customer_ids(customers_df)\n",
    "        >>> invalid_customer_ids_df.show()\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if df is None or column_name not in df.columns:\n",
    "            print(\"Invalid DataFrame or column name.\")\n",
    "            return None\n",
    "        \n",
    "        # Convert invalid values list to a set for efficient lookup\n",
    "        invalid_values_set = set(invalid_values)\n",
    "        \n",
    "        # Create the filter condition for invalid customer_ids\n",
    "        filter_condition = F.col(column_name).isin(invalid_values_set)\n",
    "        \n",
    "        # Apply the filter and return rows that match the condition\n",
    "        df_invalid = df.filter(filter_condition)\n",
    "\n",
    "        return df_invalid\n",
    "    except Exception as e:\n",
    "        print(f\"Error in function: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e06e2577-9f4c-476e-b053-ba79b19a723d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-----------------+\n",
      "|customer_id|       customer_name|             city|\n",
      "+-----------+--------------------+-----------------+\n",
      "|  1000000.0|                null|        polokwane|\n",
      "|     -99999|         PROPEL MART|        Cape Town|\n",
      "|  1000000.0|                null|        Polokwane|\n",
      "|     -99999|        Sorefoz Mart|      pretoria 14|\n",
      "|  1000000.0|          Elite Mart|      East London|\n",
      "|  1000000.0|        logic stores|        kimberley|\n",
      "|     -99999|      propel mart_11|             null|\n",
      "|     -99999|         Expert Mart|        nelspruit|\n",
      "|     -99999|   Chiptec Stores_42|      cape town 9|\n",
      "|     -99999|       lotus mart 74|        Kimberley|\n",
      "|  1000000.0|        logic stores|        Nelspruit|\n",
      "|  1000000.0|            COOLBLUE|           durban|\n",
      "|     -99999|expression stores_62|        Polokwane|\n",
      "|  1000000.0|          ELITE MART|     Nelspruit 18|\n",
      "|     -99999|            COOLBLUE|INVALID_CITY62618|\n",
      "|     -99999|          Lotus Mart|        Cape Town|\n",
      "|     -99999|        Lotus Mart10|     east london8|\n",
      "|     -99999|         expert mart|        cape town|\n",
      "|  1000000.0|         info stores|      Nelspruit11|\n",
      "|     -99999|        INVALID_NAME|      pretoria 17|\n",
      "+-----------+--------------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'drop_invalid_customer_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m invalid_customer_ids_df\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Once verified, drop rows with invalid customer_id values\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m cleaned_customers_df \u001b[38;5;241m=\u001b[39m \u001b[43mdrop_invalid_customer_ids\u001b[49m(customers_df)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Verifying the result after dropping invalid rows\u001b[39;00m\n\u001b[1;32m     11\u001b[0m cleaned_customers_df\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'drop_invalid_customer_ids' is not defined"
     ]
    }
   ],
   "source": [
    "# Show rows with invalid customer_id values first\n",
    "invalid_customer_ids_df = show_invalid_customer_ids(customers_df)\n",
    "\n",
    "# Verifying the invalid rows\n",
    "invalid_customer_ids_df.show()\n",
    "\n",
    "# Once verified, drop rows with invalid customer_id values\n",
    "cleaned_customers_df = drop_invalid_customer_ids(customers_df)\n",
    "\n",
    "# Verifying the result after dropping invalid rows\n",
    "bcleaned_customers_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f8cb62bd-1444-40ee-8042-19458a86cbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hashing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "909acbf4-3c7a-43b7-a3c8-bba0d741302c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records processed: 200\n",
      "\n",
      "Schema of final dataframe:\n",
      "root\n",
      " |-- customer_id_hash: string (nullable = false)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n",
      "\n",
      "Sample of processed data:\n",
      "+--------------------------------+-------------+-----------------+\n",
      "|customer_id_hash                |customer_name|city             |\n",
      "+--------------------------------+-------------+-----------------+\n",
      "|1e7746579353fc48ce01dad8221156c9|Info Stores  |Polokwane 11     |\n",
      "|c270645522c203f91626a17998317cc8|null         |durban           |\n",
      "|b979555b640767d3f2603e29319024f0|Coolblue 96  |Cape Town        |\n",
      "|6d9be2294eb49598988c36e46e3d630a|Logic Stores |port elizabeth-10|\n",
      "|4ac660ea4ac050c24e503263f4e29107|coolblue     |port elizabeth   |\n",
      "+--------------------------------+-------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import md5, concat, col, lit, coalesce\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# 1. Optimize by selecting only required columns\n",
    "customers_df_opt = customers_df.select(\"customer_name\", \"city\")\n",
    "\n",
    "# 2. Create MD5 hash using concatenated values\n",
    "customers_with_hash = customers_df_opt.withColumn(\n",
    "    \"customer_id_hash\",\n",
    "    md5(concat(\n",
    "        coalesce(col(\"customer_name\"), lit(\"\")),  # Handle NULL values\n",
    "        lit(\"_\"),  # Delimiter for better uniqueness\n",
    "        coalesce(col(\"city\"), lit(\"\"))\n",
    "    ))\n",
    ")\n",
    "\n",
    "# 3. Cache the result if you'll be using it multiple times\n",
    "# 3. Cache the result if you'll be using it multiple times\n",
    "customers_with_hash.cache()\n",
    "\n",
    "# 4. Perform any additional operations with the hashed data\n",
    "# For example, select final columns needed\n",
    "final_df = customers_with_hash.select(\n",
    "    \"customer_id_hash\",\n",
    "    \"customer_name\",\n",
    "    \"city\"\n",
    "\n",
    "\n",
    "# 5. Write the results (if needed)\n",
    "# Use appropriate partitioning based on your data size\n",
    "final_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"city\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .save(\"output_path\")\n",
    "\n",
    "# 6. To verify the results (optional)\n",
    "print(\"Number of records processed:\", final_df.count())\n",
    "print(\"\\nSchema of final dataframe:\")\n",
    "final_df.printSchema()\n",
    "\n",
    "# 7. Show a sample of the results (optional)\n",
    "print(\"\\nSample of processed data:\")\n",
    "final_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e99debc7-c378-4479-9bdc-39306a6e76fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "###remove numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6a086fea-846e-4e38-bd20-a613d7583ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+-----------------+\n",
      "|customer_id|    customer_name|             city|\n",
      "+-----------+-----------------+-----------------+\n",
      "|     789221|      Info Stores|        Polokwane|\n",
      "|     789301|             null|           Durban|\n",
      "|     789121|      Coolblue 96|        Cape Town|\n",
      "|     789501|     Logic Stores|   Port Elizabeth|\n",
      "|     789201|         coolblue|   Port Elizabeth|\n",
      "|     789422|    viveks stores|             null|\n",
      "|     789603|        Rel Fresh|      East London|\n",
      "|  1000000.0|             null|        Polokwane|\n",
      "|     789201|        rel fresh|             null|\n",
      "|     789303|             null|INVALID_CITY20499|\n",
      "|     123.45|             null|INVALID_CITY55754|\n",
      "|     789520|       Lotus Mart|             null|\n",
      "|     789902|    Viveks Stores|      East London|\n",
      "|     789522|    Elite Mart_74|INVALID_CITY58916|\n",
      "|     789101|    Viveks Stores|          durban1|\n",
      "|         ID|     elite mart98|         Pretoria|\n",
      "|     789301|    viveks stores|     East London1|\n",
      "|     789420|Acclaimed Stores7|        Polokwane|\n",
      "|     789420|             null|        Polokwane|\n",
      "|     789303|             null|INVALID_CITY65781|\n",
      "+-----------+-----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace, initcap, when, col, lower\n",
    "\n",
    "# Reference cities list\n",
    "cities = [\n",
    "    \"Johannesburg\", \"Cape Town\", \"Durban\", \"Pretoria\", \"Port Elizabeth\", \n",
    "    \"East London\", \"Bloemfontein\", \"Nelspruit\", \"Polokwane\", \"Kimberley\"\n",
    "]\n",
    "\n",
    "# Create a broadcast variable for efficient lookup\n",
    "cities_broadcast = spark.sparkContext.broadcast([city.lower() for city in cities])\n",
    "\n",
    "# Clean and standardize cities in one pass\n",
    "cleaned_customers_df = customers_df.withColumn(\n",
    "    \"city\",\n",
    "    when(\n",
    "        lower(regexp_replace(\n",
    "            regexp_replace(col(\"city\"), r\"[-_]\\d+$|[\\s]+\\d+$\", \"\"),  # Remove numeric noise\n",
    "            r\"\\s+\", \" \"  # Standardize spaces\n",
    "        )).isin(cities_broadcast.value),\n",
    "        initcap(regexp_replace(col(\"city\"), r\"[-_]\\d+$|[\\s]+\\d+$\", \"\"))\n",
    "    ).otherwise(col(\"city\"))\n",
    ")\n",
    "\n",
    "# Show results\n",
    "cleaned_customers_df.select(\"customer_id\", \"customer_name\", \"city\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "789626e7-52f7-47d8-a688-484c60de4257",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing null values and Invali_city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "22180197-4b60-445f-bfcd-23ff56e7a5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+-----------------+\n",
      "|    customer_name|customer_id|             city|\n",
      "+-----------------+-----------+-----------------+\n",
      "|      Info Stores|     789221|        Polokwane|\n",
      "|          unknown|     789301|           Durban|\n",
      "|      Coolblue 96|     789121|        Cape Town|\n",
      "|     Logic Stores|     789501|   Port Elizabeth|\n",
      "|         coolblue|     789201|   Port Elizabeth|\n",
      "|    viveks stores|     789422|          unknown|\n",
      "|        Rel Fresh|     789603|      East London|\n",
      "|          unknown|  1000000.0|        Polokwane|\n",
      "|        rel fresh|     789201|          unknown|\n",
      "|          unknown|     789303|INVALID_CITY20499|\n",
      "|          unknown|     123.45|INVALID_CITY55754|\n",
      "|       Lotus Mart|     789520|          unknown|\n",
      "|    Viveks Stores|     789902|      East London|\n",
      "|    Elite Mart_74|     789522|INVALID_CITY58916|\n",
      "|    Viveks Stores|     789101|          durban1|\n",
      "|     elite mart98|         ID|         Pretoria|\n",
      "|    viveks stores|     789301|     East London1|\n",
      "|Acclaimed Stores7|     789420|        Polokwane|\n",
      "|          unknown|     789420|        Polokwane|\n",
      "|          unknown|     789303|INVALID_CITY65781|\n",
      "+-----------------+-----------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace, initcap, when, col, lower, lit\n",
    "\n",
    "# Reference cities list\n",
    "cities = [\n",
    "    \"Johannesburg\", \"Cape Town\", \"Durban\", \"Pretoria\", \"Port Elizabeth\", \n",
    "    \"East London\", \"Bloemfontein\", \"Nelspruit\", \"Polokwane\", \"Kimberley\"\n",
    "]\n",
    "\n",
    "# Create a broadcast variable for efficient lookup\n",
    "cities_broadcast = spark.sparkContext.broadcast([city.lower() for city in cities])\n",
    "\n",
    "# Clean and standardize cities, handle null names and invalid cities\n",
    "cleaned_customers_df = customers_df.withColumn(\n",
    "   \"customer_name\",\n",
    "    when(col(\"customer_name\").isNull(), lit(\"unknown\"))\n",
    "    .otherwise(col(\"customer_name\"))\n",
    ").withColumn(\n",
    "    \"city\",\n",
    "    when(\n",
    "        (lower(col(\"city\")).like(\"INVALID_CITY%\")) |  # Match INVALID_CITY pattern\n",
    "        (col(\"city\").isNull()),                       # Handle null cities\n",
    "        lit(\"unknown\")\n",
    "    ).otherwise(\n",
    "        when(\n",
    "            lower(regexp_replace(\n",
    "                regexp_replace(col(\"city\"), r\"[-_]\\d+$|[\\s]+\\d+$\", \"\"),\n",
    "                r\"\\s+\", \" \"\n",
    "            )).isin(cities_broadcast.value),\n",
    "            initcap(regexp_replace(col(\"city\"), r\"[-_]\\d+$|[\\s]+\\d+$\", \"\"))\n",
    "        ).otherwise(col(\"city\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "# Show results\n",
    "cleaned_customers_df.select(\"customer_name\", \"customer_id\", \"city\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c7f2a1a9-8a6a-465c-97c0-4f1220e0cce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+--------------+\n",
      "|customer_id|customer_name    |city          |\n",
      "+-----------+-----------------+--------------+\n",
      "|789221     |Info Stores      |Polokwane     |\n",
      "|789301     |Unknown          |Durban        |\n",
      "|789121     |Coolblue 96      |Cape Town     |\n",
      "|789501     |Logic Stores     |Port Elizabeth|\n",
      "|789201     |coolblue         |Port Elizabeth|\n",
      "|789422     |viveks stores    |Unknown       |\n",
      "|789603     |Rel Fresh        |East London   |\n",
      "|1000000.0  |Unknown          |Polokwane     |\n",
      "|789201     |rel fresh        |Unknown       |\n",
      "|789303     |Unknown          |Unknown       |\n",
      "|123.45     |Unknown          |Unknown       |\n",
      "|789520     |Lotus Mart       |Unknown       |\n",
      "|789902     |Viveks Stores    |East London   |\n",
      "|789522     |Elite Mart_74    |Unknown       |\n",
      "|789101     |Viveks Stores    |durban1       |\n",
      "|ID         |elite mart98     |Pretoria      |\n",
      "|789301     |viveks stores    |East London1  |\n",
      "|789420     |Acclaimed Stores7|Polokwane     |\n",
      "|789420     |Unknown          |Polokwane     |\n",
      "|789303     |Unknown          |Unknown       |\n",
      "+-----------+-----------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Remaining invalid cities count: 0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace, initcap, when, col, lower, lit, upper\n",
    "\n",
    "# Reference cities list\n",
    "cities = [\n",
    "    \"Johannesburg\", \"Cape Town\", \"Durban\", \"Pretoria\", \"Port Elizabeth\", \n",
    "    \"East London\", \"Bloemfontein\", \"Nelspruit\", \"Polokwane\", \"Kimberley\"\n",
    "]\n",
    "\n",
    "# Create a broadcast variable for efficient lookup\n",
    "cities_broadcast = spark.sparkContext.broadcast([city.lower() for city in cities])\n",
    "\n",
    "# Clean and standardize cities, handle null names and invalid cities\n",
    "cleaned_customers_df = customers_df.withColumn(\n",
    "    \"customer_name\",\n",
    "    when(col(\"customer_name\").isNull(), lit(\"Unknown\"))\n",
    "    .otherwise(col(\"customer_name\"))\n",
    ").withColumn(\n",
    "    \"city\",\n",
    "    when(\n",
    "        (upper(col(\"city\")).rlike(\"INVALID_CITY[0-9]*\")) |  # Match any case of INVALID_CITY followed by numbers\n",
    "        (col(\"city\").isNull()),\n",
    "        lit(\"Unknown\")\n",
    "    ).otherwise(\n",
    "        when(\n",
    "            lower(regexp_replace(\n",
    "                regexp_replace(col(\"city\"), r\"[-_]\\d+$|[\\s]+\\d+$\", \"\"),\n",
    "                r\"\\s+\", \" \"\n",
    "            )).isin(cities_broadcast.value),\n",
    "            initcap(regexp_replace(col(\"city\"), r\"[-_]\\d+$|[\\s]+\\d+$\", \"\"))\n",
    "        ).otherwise(col(\"city\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "# Show results\n",
    "cleaned_customers_df.select( \"customer_id\",\"customer_name\",\"city\").show(truncate=False)\n",
    "\n",
    "# Optional: Check if any INVALID_CITY patterns remain\n",
    "remaining_invalid = cleaned_customers_df.filter(upper(col(\"city\")).rlike(\"INVALID_CITY\"))\n",
    "print(\"Remaining invalid cities count:\", remaining_invalid.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "43f4eac3-001b-4479-950c-82b095698633",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cHECKING FOR DUPLICATES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d4d0022e-4432-4f84-99e3-0e8c28385bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking for duplicates...\n",
      "Number of duplicate records found: 4\n",
      "\n",
      "Duplicate records found:\n",
      "+-----------+-------------+---------+-----+\n",
      "|customer_id|customer_name|city     |count|\n",
      "+-----------+-------------+---------+-----+\n",
      "|1000000.0  |Unknown      |Polokwane|2    |\n",
      "|123.45     |Unknown      |Unknown  |3    |\n",
      "|789303     |Unknown      |Unknown  |2    |\n",
      "|789122     |logic stores |Cape Town|2    |\n",
      "+-----------+-------------+---------+-----+\n",
      "\n",
      "\n",
      "Original record count: 200\n",
      "Record count after removing duplicates: 195\n",
      "\n",
      "Final cleaned and deduplicated data:\n",
      "+-----------+----------------+--------------+\n",
      "|customer_id|customer_name   |city          |\n",
      "+-----------+----------------+--------------+\n",
      "|1000000.0  |Unknown         |Polokwane     |\n",
      "|789420     |Acclaimed Stores|Unknown       |\n",
      "|abcd       |Propel Mart     |Nelspruit     |\n",
      "|789320     |Propel Mart_95  |Unknown       |\n",
      "|123.45     |Rel Fresh       |East London   |\n",
      "|789201     |Sorefoz Mart    |Durban        |\n",
      "|789102     |Lotus Mart      |Nelspruit     |\n",
      "|789503     |Unknown         |Bloemfontein  |\n",
      "|789121     |Elite Mart      |Nelspruit     |\n",
      "|789421     |Unknown         |Cape Town     |\n",
      "|789501     |vijay stores    |Polokwane     |\n",
      "|789521     |Viveks Stores   |Pretoria10    |\n",
      "|789401     |Unknown         |Pretoria      |\n",
      "|789420     |Unknown         |Port Elizabeth|\n",
      "|789422     |CHIPTEC STORES  |Unknown       |\n",
      "|789622     |chiptec stores  |Unknown       |\n",
      "|789121     |Propel Mart_80  |Kimberley     |\n",
      "|789421     |Viveks Stores_15|Unknown       |\n",
      "|789603     |expert mart     |Unknown       |\n",
      "|789603     |Vijay Stores    |East London   |\n",
      "+-----------+----------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace, initcap, when, col, lower, lit, upper, count\n",
    "\n",
    "# Your existing cleaning code remains the same up to the cleaned_customers_df creation\n",
    "\n",
    "# First, let's check for duplicates\n",
    "print(\"\\nChecking for duplicates...\")\n",
    "duplicate_check = cleaned_customers_df.groupBy(\"customer_id\", \"customer_name\", \"city\") \\\n",
    "    .agg(count(\"*\").alias(\"count\")) \\\n",
    "    .filter(col(\"count\") > 1)\n",
    "\n",
    "print(\"Number of duplicate records found:\", duplicate_check.count())\n",
    "\n",
    "# Show duplicate records if any exist\n",
    "if duplicate_check.count() > 0:\n",
    "    print(\"\\nDuplicate records found:\")\n",
    "    duplicate_check.show(truncate=False)\n",
    "\n",
    "# Remove duplicates\n",
    "deduped_customers_df = cleaned_customers_df.dropDuplicates([\"customer_id\", \"customer_name\", \"city\"])\n",
    "\n",
    "print(\"\\nOriginal record count:\", cleaned_customers_df.count())\n",
    "print(\"Record count after removing duplicates:\", deduped_customers_df.count())\n",
    "\n",
    "# Show final results\n",
    "print(\"\\nFinal cleaned and deduplicated data:\")\n",
    "deduped_customers_df.select(\"customer_id\", \"customer_name\", \"city\").show(truncate=False)\n",
    "\n",
    "# Save the final cleaned and deduplicated dataframe\n",
    "deduped_customers_df.createOrReplaceTempView(\"cleaned_customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c04c02-688c-484c-8505-d4e1397300f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
